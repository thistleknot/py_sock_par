{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c842ae64-4502-42d2-aeab-22d1cd96e13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install clustergram pandas_profiling scipy sklearn statsmodels IPython dtale matplotlib rpy2 seaborn shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf49edd-ccdb-4a5a-9ede-a9b3c081ceb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#put in ~/.bashrc\n",
    "#LD_PRELOAD=\"/mnt/distvol/R/4.1.2/lib64/R/lib/LibR.so\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab96046e-bb6b-420f-b6b1-9524e60fe678",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from fracdiff import fdiff\n",
    "#import urbangrammar-graphics as ugg\n",
    "import os\n",
    "from clustergram import Clustergram\n",
    "from concurrent.futures import wait, ALL_COMPLETED\n",
    "from dask.distributed import Client\n",
    "from dask.distributed import as_completed\n",
    "from numpy import absolute\n",
    "from numpy import arange\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from pandas_profiling import ProfileReport\n",
    "#from rpy2.robjects import pandas2ri\n",
    "from rpy2.robjects.conversion import localconverter\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.rinterface_lib import openrlib\n",
    "from scipy import stats\n",
    "from scipy.cluster.vq import vq\n",
    "from scipy.spatial.distance import cdist, pdist\n",
    "from scipy.special import boxcox, inv_boxcox\n",
    "from scipy.stats import f\n",
    "from sklearn import preprocessing\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import *\n",
    "#from sklearn.preprocessing import PowerTransformer\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn.preprocessing import scale\n",
    "from sklearn.utils import as_float_array\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from statsmodels.stats.outliers_influence import OLSInfluence\n",
    "import IPython\n",
    "import concurrent.futures\n",
    "import dask.dataframe as dd\n",
    "import datetime\n",
    "import dtale\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pingouin as pg\n",
    "import re\n",
    "import rpy2\n",
    "import rpy2.robjects as ro\n",
    "import rpy2.situation\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import sklearn\n",
    "import sklearn.linear_model\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tools\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693a8c73-5c94-4d9a-bf1b-9ffbe6836975",
   "metadata": {},
   "outputs": [],
   "source": [
    "#c = get_config()\n",
    "libpath = os.environ.get('LD_LIBRARY_PATH', '')\n",
    "os.environ['LD_LIBRARY_PATH'] = (\n",
    "    rpy2.situation.r_ld_library_path_from_subprocess(openrlib.R_HOME) +\n",
    "    libpath\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c465a19f-5039-4627-8b7b-23d0b18f6017",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testNormal (x):    \n",
    "    \n",
    "    k2, p = stats.normaltest(x)\n",
    "    alpha = .001\n",
    "    #print(\"p = {:g}\".format(p))    \n",
    "    if p < alpha:  # null hypothesis: x comes from a normal distribution\n",
    "        #print(p)\n",
    "        #print(alpha)\n",
    "        print(\"The null hypothesis can be rejected\")\n",
    "        xt, _ = stats.yeojohnson(x)\n",
    "        #xt, _ = stats.boxcox(x)        \n",
    "        print(_)\n",
    "        xt = pd.DataFrame(xt)\n",
    "        \n",
    "        return _, pd.DataFrame(xt).set_index(x.index)\n",
    "    else:\n",
    "        print(\"The null hypothesis cannot be rejected\")    \n",
    "        return 1, pd.DataFrame(x)\n",
    "\n",
    "def inverse_boxcox (data, lambdas):\n",
    "    power = PowerTransformer(method='yeo-johnson')\n",
    "    power.lambdas_ = lambdas.values\n",
    "    return(power.inverse_transform([data]))\n",
    "    #return inv_boxcox(data, lambdas.values)\n",
    "    \n",
    "def transform_boxcox_l(data, l_):\n",
    "    transformed = pd.DataFrame()\n",
    "\n",
    "    for i in range(0,len(data.columns)):\n",
    "        #print(i)\n",
    "        if l_.iloc[i].values == 1:\n",
    "            inner_scale = data.iloc[:,i]            \n",
    "        else:\n",
    "            inner_scale = pd.DataFrame(stats.yeojohnson((data.iloc[:,i]), lmbda=l_.iloc[i].values))\n",
    "            \n",
    "        inner_scale.index = data.index\n",
    "        transformed = pd.concat([transformed,inner_scale],axis=1)\n",
    "        \n",
    "    transformed.columns = data.columns\n",
    "    return transformed\n",
    "\n",
    "def transform_boxcox (data):\n",
    "    transformed = pd.DataFrame()\n",
    "    transformed_lambdas = pd.DataFrame()\n",
    "\n",
    "    for i in range(0,len(data.columns)):\n",
    "        l, inner_scale = testNormal(data.iloc[:,i])\n",
    "        inner_scale.set_index(data.index)\n",
    "\n",
    "        transformed_lambdas = pd.concat([transformed_lambdas,pd.DataFrame(pd.Series(l))],axis=0)\n",
    "        transformed = pd.concat([transformed,inner_scale],axis=1)\n",
    "        \n",
    "    transformed.columns = data.columns\n",
    "    return transformed, transformed_lambdas\n",
    "\n",
    "def inverse_yeo(og, data_, lambda_):\n",
    "    values = []\n",
    "    for i in range(0,len(og)):\n",
    "        X = og[i]\n",
    "        X_trans = data_[i]\n",
    "        if X >= 0 and lambda_ == 0:\n",
    "            X = exp(X_trans) - 1\n",
    "        elif X >= 0 and lambda_ != 0:\n",
    "            X = (X_trans * lambda_ + 1) ** (1 / lambda_) - 1\n",
    "        elif X < 0 and lambda_ != 2:\n",
    "            X = 1 - (-(2 - lambda_) * X_trans + 1) ** (1 / (2 - lambda_))\n",
    "        elif X < 0 and lambda_ == 2:\n",
    "            X = 1 - exp(-X_trans)\n",
    "        \n",
    "        values.append(X)\n",
    "    return(pd.DataFrame(values))\n",
    "\n",
    "\n",
    "def revert_yeo (og, data_, lambdas):\n",
    "    reverted = pd.DataFrame()\n",
    "\n",
    "    for i in range(0,len(data_.columns)):        \n",
    "        if lambdas.iloc[i].values == 1 :\n",
    "            revert = data_.iloc[:,i]\n",
    "        else:\n",
    "            p#ower = PowerTransformer(method='yeo-johnson')\n",
    "            #power.lambdas_ = lambdas.iloc[i].values\n",
    "            #revert = pd.DataFrame(power.inverse_transform([data.iloc[:,i].values]))\n",
    "            #return inv_boxcox(data, lambdas.values)\n",
    "            revert = pd.DataFrame(inverse_yeo(og.iloc[:,i].values,data_.iloc[:,i].values, lambdas.iloc[i].values))            \n",
    "        revert.index = data_.index\n",
    "        reverted = pd.concat([reverted,revert],axis=1)\n",
    "        \n",
    "    reverted.columns = data_.columns\n",
    "    return reverted\n",
    "\n",
    "class ZCA(BaseEstimator, TransformerMixin):\n",
    "  def __init__(self, regularization=1e-5, copy=False):\n",
    "      self.regularization = regularization\n",
    "      self.copy = copy\n",
    "  def fit(self, X, y=None):\n",
    "      X = as_float_array(X, copy=self.copy)\n",
    "      self.mean_ = np.mean(X, axis=0)\n",
    "      X = X - self.mean_\n",
    "      sigma = np.dot(X.T, X) / (X.shape[0] - 1)\n",
    "      U, S, V = np.linalg.svd(sigma)\n",
    "      tmp = np.dot(U, np.diag(1 / np.sqrt(S + self.regularization)))\n",
    "      self.components_ = np.dot(tmp, U.T)\n",
    "      return self\n",
    "  def transform(self, X):\n",
    "      X_transformed = X - self.mean_\n",
    "      X_transformed = np.dot(X_transformed, self.components_.T)\n",
    "      return X_transformed  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33a75df-bb32-49a6-9fb6-1275c50118bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "zca = ZCA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f66c72-7f7a-4bed-b589-234d2844ad9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98273a1b-c6c6-47df-8600-b942e9a9b095",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pd.read_csv(\"all_data.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241d4832-d71e-466b-97e1-e5d09789e20d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f8aa6d-17f5-4d4d-8529-f974437f2402",
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw = pd.read_csv(\"all_data.csv\",index_col=0)\n",
    "raw = dd.read_csv(\"all_data.csv\").set_index('Unnamed: 0')\n",
    "#raw = dd.read_csv(\"all_data.csv\")\n",
    "#raw = dd.read_csv(\"all_data.csv\",index_col=0)\n",
    "#dd.set_index(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b74c5ae-5050-4b8a-bc89-0a8afdd7b864",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.datetime(raw.index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8b7161-dab2-4644-ac81-abaa24776f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_dd = dd.DataFrame(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad98b43-60bd-45bc-8a1a-8b0f03a37e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd69546e-e239-47d5-9cae-17b9d4492b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_int = raw.interpolate(method='time')\n",
    "raw_int.dropna(inplace=True)\n",
    "#.fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3708ee71-dd79-4be6-9921-5f92eed6b6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_int.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e750e9ea-0a15-4d62-ad60-3cdf0afd749f",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = (raw_int-raw_int.shift()).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce45a309-a12d-4f2f-99bf-0a541ac2f614",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d209f7ca-b3e6-4184-9255-4c8cc7cf10f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af48ec2-dda3-453e-ae5f-5d4a994dfa68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d33c54e-9bd1-4012-b11a-6a28f1cc8198",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def train(partition):\n",
    "    est = LinearRegression()\n",
    "    est.fit(partition[['x']].values, partition.y.values)\n",
    "    return est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f0d63e-85ca-449b-8390-aeeecb71313f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "target = all_data.columns[0]\n",
    "\n",
    "\n",
    "\n",
    "scaler.fit(np.array(all_data[target]).reshape(-1, 1))\n",
    "\n",
    "num_folds = 10\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "kfold.get_n_splits(all_data.index)\n",
    "\n",
    "exclude = 'States'\n",
    "\n",
    "sig_table = np.zeros(shape=(len(all_data.columns)))\n",
    "signs_table = np.zeros(shape=(len(all_data.columns)))\n",
    "\n",
    "p_threshold = .05\n",
    "\n",
    "New_Names = all_data.columns[2:]\n",
    "iteration = 0\n",
    "for train_index, test_index in kfold.split(all_data):\n",
    "    #print(iteration)\n",
    "    max_pvalue = 1\n",
    "    \n",
    "    subset = all_data.iloc[train_index].loc[:, ~all_data.columns.isin([exclude])]\n",
    "    \n",
    "    #skip y and states\n",
    "    set_ = subset.loc[:, ~subset.columns.isin([target])].columns.tolist()\n",
    "    \n",
    "    n=len(subset)\n",
    "    \n",
    "    while(max_pvalue>=.05):\n",
    "\n",
    "        dist = scipy.stats.beta(n/2 - 1, n/2 - 1, loc=-1, scale=2)\n",
    "        p_values = pd.DataFrame(2*dist.cdf(-abs(subset.pcorr()[target]))).T\n",
    "        p_values.columns = list(subset.columns)\n",
    "        \n",
    "        max_pname = p_values.idxmax(axis=1)[0]\n",
    "        max_pvalue = p_values[max_pname].values[0]\n",
    "        \n",
    "        if (max_pvalue > .05):\n",
    "\n",
    "            set_.remove(max_pname)\n",
    "            temp = [target]\n",
    "            temp.extend(set_)\n",
    "            subset = subset[temp]\n",
    "    \n",
    "    winners = p_values.loc[:, ~p_values.columns.isin([target])].columns.tolist()\n",
    "    sig_table = (sig_table + np.where(all_data.columns.isin(winners),1,0)).copy()\n",
    "    \n",
    "    signs_table[all_data.columns.get_indexer(winners)]+=np.where(subset.pcorr()[target][winners]<0,-1,1)\n",
    "\n",
    "\n",
    "significance = pd.DataFrame(sig_table).T\n",
    "significance.columns = list(all_data.columns)\n",
    "display(significance)\n",
    "\n",
    "sign = pd.DataFrame(signs_table).T\n",
    "sign.columns = list(all_data.columns)\n",
    "display(sign)\n",
    "\n",
    "purity = abs((sign/num_folds)*(sign/significance)).T.replace([np.inf, -np.inf, np.NaN], 0)\n",
    "display(purity.T)\n",
    "\n",
    "threshold = .5\n",
    "\n",
    "chosen = list(purity.T.columns.values[np.array(purity.T>=threshold).reshape(len(all_data.columns,))])\n",
    "dataSet = pd.concat([all_data[target],all_data[chosen]],axis=1)\n",
    "y_scaled = pd.DataFrame(scaler.transform(np.array(dataSet[target]).reshape(-1, 1)))\n",
    "y_scaled.columns=[target]\n",
    "\n",
    "display(chosen)\n",
    "\n",
    "zca_data = pd.concat([y_scaled,pd.DataFrame(zca.fit_transform(dataSet[chosen]),columns=chosen)],axis=1)\n",
    "\n",
    "zca_data.pcorr()\n",
    "\n",
    "sns.pairplot(zca_data)\n",
    "\n",
    "\n",
    "#model = sklearn.linear_model.LinearRegression()\n",
    "data_set_wConstant = statsmodels.tools.tools.add_constant(zca_data)\n",
    "y = data_set_wConstant[target]\n",
    "X = data_set_wConstant[data_set_wConstant.columns.drop(target)]\n",
    "#results = model.fit(X, y)\n",
    "model = sm.OLS(y,X)\n",
    "results = model.fit()\n",
    "\n",
    "results.summary()\n",
    "\n",
    "pd.DataFrame(results.get_influence().resid_studentized_internal).hist()\n",
    "\n",
    "plt.scatter(results.fittedvalues*scaler.scale_[0] + scaler.mean_[0], y*scaler.scale_[0] + scaler.mean_[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
