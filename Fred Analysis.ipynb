{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c842ae64-4502-42d2-aeab-22d1cd96e13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install clustergram pandas_profiling scipy sklearn statsmodels IPython dtale matplotlib rpy2 seaborn shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf49edd-ccdb-4a5a-9ede-a9b3c081ceb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#put in ~/.bashrc\n",
    "#LD_PRELOAD=\"/mnt/distvol/R/4.1.2/lib64/R/lib/LibR.so\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab96046e-bb6b-420f-b6b1-9524e60fe678",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from fracdiff import fdiff\n",
    "#import urbangrammar-graphics as ugg\n",
    "%matplotlib inline\n",
    "import os\n",
    "from clustergram import Clustergram\n",
    "from concurrent.futures import wait, ALL_COMPLETED\n",
    "from dask.distributed import Client\n",
    "from dask.distributed import as_completed\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as sfs\n",
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from numpy import absolute\n",
    "from numpy import arange\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from pandas_profiling import ProfileReport\n",
    "#from rpy2.robjects import pandas2ri\n",
    "from pmdarima.utils import diff_inv\n",
    "from rpy2.robjects.conversion import localconverter\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.rinterface_lib import openrlib\n",
    "from scipy import stats\n",
    "from scipy.cluster.vq import vq\n",
    "from scipy.spatial.distance import cdist, pdist\n",
    "from scipy.special import boxcox, inv_boxcox\n",
    "from scipy.stats import f\n",
    "from sklearn import preprocessing\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import *\n",
    "#from sklearn.preprocessing import PowerTransformer\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn.preprocessing import scale\n",
    "from sklearn.utils import as_float_array\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.stats.outliers_influence import OLSInfluence\n",
    "import statsmodels.api as sm\n",
    "import IPython\n",
    "import concurrent.futures\n",
    "import dask.dataframe as dd\n",
    "import datetime\n",
    "import dtale\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pingouin as pg\n",
    "import pmdarima\n",
    "import pycorrelate\n",
    "import random\n",
    "import re\n",
    "import rpy2\n",
    "import rpy2.robjects as ro\n",
    "import rpy2.situation\n",
    "import scipy\n",
    "import seaborn as sn\n",
    "import shap\n",
    "import sklearn\n",
    "import sklearn.linear_model\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tools\n",
    "import sys\n",
    "import time\n",
    "if not sys.warnoptions:\n",
    "\timport warnings\n",
    "\twarnings.simplefilter(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693a8c73-5c94-4d9a-bf1b-9ffbe6836975",
   "metadata": {},
   "outputs": [],
   "source": [
    "#c = get_config()\n",
    "libpath = os.environ.get('LD_LIBRARY_PATH', '')\n",
    "os.environ['LD_LIBRARY_PATH'] = (\n",
    "    rpy2.situation.r_ld_library_path_from_subprocess(openrlib.R_HOME) +\n",
    "    libpath\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c465a19f-5039-4627-8b7b-23d0b18f6017",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regress (dd_df):\n",
    "    \n",
    "    lr = LinearRegression()\n",
    "    print(\"y needs to be named 'target', regress only uses the variable names, it doesn't use index's.  You apply that using .fit on this functions return\")\n",
    "    #dd_df = data_final\n",
    "    variables=dd_df.columns\n",
    "    target = variables[0]\n",
    "    te = pd.DataFrame(dd_df['target']) \n",
    "    te.index = ([*dd_df.index])\n",
    "    #te.columns = ['target']\n",
    "    col_names = variables[~variables.isin(['target'])].ravel()\n",
    "\n",
    "    s_f_s = sfs(lr, \n",
    "              k_features=np.int(len(col_names)*.05), \n",
    "              forward=True, \n",
    "              floating=True, \n",
    "              scoring='neg_mean_absolute_percentage_error',\n",
    "              n_jobs=-1,\n",
    "              cv=5)\n",
    "\n",
    "    return (s_f_s)\n",
    "\n",
    "def last_day_of_month(date):\n",
    "    return date.replace(day=1) + relativedelta(months=1) - relativedelta(days=1)\n",
    "\n",
    "def findknee(xdata):\n",
    "    rate_of_change=(xdata[0]-xdata[-1])/(len(xdata)-1)\n",
    "    #print(rate_of_change)\n",
    "    delta = xdata-xdata[-1]\n",
    "    deltas = []\n",
    "    deltas.append(delta[0])\n",
    "    for d in range(1,len(xdata)):\n",
    "        deltas.append(deltas[d-1]-rate_of_change)\n",
    "    #print(deltas)\n",
    "    for d in range(0,len(xdata)):\n",
    "        deltas[d]=delta[d]-deltas[d]\n",
    "    return(np.abs(deltas))\n",
    "    \n",
    "def MAPE(Y_actual,Y_Predicted):\n",
    "    mape = np.mean(np.abs((Y_actual - Y_Predicted)/Y_actual))*100\n",
    "    return mape\n",
    "\n",
    "def formula_from_cols(df, y):\n",
    "    return y + ' ~ ' + ' + '.join([col for col in df.columns if not col==y])\n",
    "\n",
    "def testNormal (x):    \n",
    "    \n",
    "    k2, p = stats.normaltest(x)\n",
    "    alpha = .001\n",
    "    #print(\"p = {:g}\".format(p))    \n",
    "    if p < alpha:  # null hypothesis: x comes from a normal distribution\n",
    "        #print(p)\n",
    "        #print(alpha)\n",
    "        print(\"The null hypothesis can be rejected\")\n",
    "        xt, _ = stats.yeojohnson(x)\n",
    "        #xt, _ = stats.boxcox(x)        \n",
    "        print(_)\n",
    "        xt = pd.DataFrame(xt)\n",
    "        \n",
    "        return _, pd.DataFrame(xt).set_index(x.index)\n",
    "    else:\n",
    "        print(\"The null hypothesis cannot be rejected\")    \n",
    "        return 1, pd.DataFrame(x)\n",
    "\n",
    "def inverse_boxcox (data, lambdas):\n",
    "    power = PowerTransformer(method='yeo-johnson')\n",
    "    power.lambdas_ = lambdas.values\n",
    "    return(power.inverse_transform([data]))\n",
    "    #return inv_boxcox(data, lambdas.values)\n",
    "    \n",
    "def transform_boxcox_l(data, l_):\n",
    "    transformed = pd.DataFrame()\n",
    "\n",
    "    for i in range(0,len(data.columns)):\n",
    "        #print(i)\n",
    "        if l_.iloc[i].values == 1:\n",
    "            inner_scale = data.iloc[:,i]            \n",
    "        else:\n",
    "            inner_scale = pd.DataFrame(stats.yeojohnson((data.iloc[:,i]), lmbda=l_.iloc[i].values))\n",
    "            \n",
    "        inner_scale.index = data.index\n",
    "        transformed = pd.concat([transformed,inner_scale],axis=1)\n",
    "        \n",
    "    transformed.columns = data.columns\n",
    "    return transformed\n",
    "\n",
    "def transform_boxcox (data):\n",
    "    transformed = pd.DataFrame()\n",
    "    transformed_lambdas = pd.DataFrame()\n",
    "\n",
    "    for i in range(0,len(data.columns)):\n",
    "        l, inner_scale = testNormal(data.iloc[:,i])\n",
    "        inner_scale.set_index(data.index)\n",
    "\n",
    "        transformed_lambdas = pd.concat([transformed_lambdas,pd.DataFrame(pd.Series(l))],axis=0)\n",
    "        transformed = pd.concat([transformed,inner_scale],axis=1)\n",
    "        \n",
    "    transformed.columns = data.columns\n",
    "    return transformed, transformed_lambdas\n",
    "\n",
    "def inverse_yeo(og, data_, lambda_):\n",
    "    values = []\n",
    "    for i in range(0,len(og)):\n",
    "        X = og[i]\n",
    "        X_trans = data_[i]\n",
    "        if X >= 0 and lambda_ == 0:\n",
    "            X = exp(X_trans) - 1\n",
    "        elif X >= 0 and lambda_ != 0:\n",
    "            X = (X_trans * lambda_ + 1) ** (1 / lambda_) - 1\n",
    "        elif X < 0 and lambda_ != 2:\n",
    "            X = 1 - (-(2 - lambda_) * X_trans + 1) ** (1 / (2 - lambda_))\n",
    "        elif X < 0 and lambda_ == 2:\n",
    "            X = 1 - exp(-X_trans)\n",
    "        \n",
    "        values.append(X)\n",
    "    return(pd.DataFrame(values))\n",
    "\n",
    "def revert_yeo (og, data_, lambdas):\n",
    "    reverted = pd.DataFrame()\n",
    "\n",
    "    for i in range(0,len(data_.columns)):        \n",
    "        if lambdas.iloc[i].values == 1 :\n",
    "            revert = data_.iloc[:,i]\n",
    "        else:\n",
    "            p#ower = PowerTransformer(method='yeo-johnson')\n",
    "            #power.lambdas_ = lambdas.iloc[i].values\n",
    "            #revert = pd.DataFrame(power.inverse_transform([data.iloc[:,i].values]))\n",
    "            #return inv_boxcox(data, lambdas.values)\n",
    "            revert = pd.DataFrame(inverse_yeo(og.iloc[:,i].values,data_.iloc[:,i].values, lambdas.iloc[i].values))            \n",
    "        revert.index = data_.index\n",
    "        reverted = pd.concat([reverted,revert],axis=1)\n",
    "        \n",
    "    reverted.columns = data_.columns\n",
    "    return reverted\n",
    "\n",
    "class ZCA(BaseEstimator, TransformerMixin):\n",
    "  def __init__(self, regularization=1e-5, copy=False):\n",
    "      self.regularization = regularization\n",
    "      self.copy = copy\n",
    "  def fit(self, X, y=None):\n",
    "      X = as_float_array(X, copy=self.copy)\n",
    "      self.mean_ = np.mean(X, axis=0)\n",
    "      X = X - self.mean_\n",
    "      sigma = np.dot(X.T, X) / (X.shape[0] - 1)\n",
    "      U, S, V = np.linalg.svd(sigma)\n",
    "      tmp = np.dot(U, np.diag(1 / np.sqrt(S + self.regularization)))\n",
    "      self.components_ = np.dot(tmp, U.T)\n",
    "      return self\n",
    "  def transform(self, X):\n",
    "      X_transformed = X - self.mean_\n",
    "      X_transformed = np.dot(X_transformed, self.components_.T)\n",
    "      return X_transformed\n",
    "\n",
    "def crosscorrelation(x, y, maxlag, mode='corr'):\n",
    "    \"\"\"\n",
    "    Cross correlation with a maximum number of lags.\n",
    "\n",
    "    `x` and `y` must be one-dimensional numpy arrays with the same length.\n",
    "\n",
    "    This computes the same result as\n",
    "        numpy.correlate(x, y, mode='full')[len(a)-maxlag-1:len(a)+maxlag]\n",
    "\n",
    "    The return vaue has length 2*maxlag + 1.\n",
    "    \"\"\"\n",
    "    py = np.pad(y.conj(), 2*maxlag, mode='constant')\n",
    "    T = np.lib.stride_tricks.as_strided(py[2*maxlag:], shape=(2*maxlag+1, len(y) + 2*maxlag),\n",
    "                   strides=(-py.strides[0], py.strides[0]))\n",
    "    px = np.pad(x, maxlag, mode='constant')\n",
    "    if mode == 'dot':       # get lagged dot product\n",
    "        return T.dot(px)\n",
    "    elif mode == 'corr':    # gets Pearson correlation\n",
    "        return (T.dot(px)/px.size - (T.mean(axis=1)*px.mean())) / \\\n",
    "               (np.std(T, axis=1) * np.std(px)) \n",
    "    \n",
    "def ret_ccf(npa_):\n",
    "    y_name = npa_[0]\n",
    "    x_name = npa_[1]\n",
    "    index = npa_[2]\n",
    "    \n",
    "    data = cleaned.loc[index].dropna()\n",
    "    \n",
    "    y = np.array(data.iloc[:,data.columns==y_name]).ravel()\n",
    "    \n",
    "    x = np.array(data.iloc[:,data.columns==x_name]).ravel()\n",
    "    #print(x)\n",
    "    #ccf = statsmodels.tsa.stattools.ccf(x,y)\n",
    "    ccf = crosscorrelation(x,y, ccf_max_lag, mode='corr')\n",
    "    #print(ccf)\n",
    "    return([y_name,x_name,ccf])\n",
    "\n",
    "def train(partition):\n",
    "    est = LinearRegression()\n",
    "    est.fit(partition[New_Names].values, partition['target'])\n",
    "    return est\n",
    "\n",
    "    \n",
    "    '''\n",
    "    \n",
    "def nv_diff_sets(v_of_i,dataset,f_casts):\n",
    "\n",
    "  s_=sndif_[which(colnames(raw)==var_of_int)]\n",
    "  d_=ndif_[which(colnames(raw)==var_of_int)]\n",
    "  \n",
    "  startRow = c()\n",
    "  for (r in rownames(dataset[1:d_,,drop=FALSE])):\n",
    "    startRow = c(startRow,which(rownames(raw)==r))\n",
    "  \n",
    "  data_ = c(na.omit(c(dataset[,var_of_int], f_casts)))\n",
    "  \n",
    "  if(s_==0):\n",
    "    inv_d = diffinv(data_,differences=d_,xi=raw[startRow,var_of_int])\n",
    "  else:  \n",
    "    inv_d = diffinv(diffinv(data_,differences = d_, xi=raw[startRow,var_of_int]), differences = s_,xi=raw[startRow:(startRow+season-1),var_of_int])\n",
    "    \n",
    "  return(inv_d)\n",
    "'''\n",
    "\n",
    "def lagpad(x, k):\n",
    "    length=np.full(abs(k), np.NaN)\n",
    "    #print(length)\n",
    "    #k=k-1\n",
    "    if (k>0):\n",
    "        result = np.concatenate([length,x[0:(len(x)-k)]])\n",
    "    elif (k<0):\n",
    "        result= np.concatenate([(x[abs(k):(len(x))]),length])\n",
    "    else:\n",
    "        result= x\n",
    "    return(result)\n",
    "\n",
    "def lag(data):\n",
    "    return lagpad(data,1)\n",
    "\n",
    "def sndif_(npa_):\n",
    "    index = npa_[2]\n",
    "    #print(index)\n",
    "    data = raw_int[npa_[0]].loc[index]\n",
    "    #print(data)\n",
    "    return(pmdarima.arima.nsdiffs(data.dropna(),m=npa_[1]))\n",
    "\n",
    "def ndif_(npa_):\n",
    "    index = npa_[1]\n",
    "    data = raw_int[npa_[0]].loc[index]\n",
    "\n",
    "    score = pmdarima.arima.ndiffs(data.dropna())\n",
    "    \n",
    "    if(score==0):\n",
    "        score = 1\n",
    "    return(score)\n",
    "\n",
    "def clientFunction(function_name,npa):\n",
    "    client = Client('192.168.3.100:8786',timeout=3)\n",
    "    future = client.map(function_name,npa)\n",
    "\n",
    "    results = []\n",
    "    for f in as_completed(future):\n",
    "        if(f.status==\"error\"):\n",
    "            results.append(\"error\")\n",
    "        else:\n",
    "            results.append(f.result())   \n",
    "\n",
    "    client.close()\n",
    "\n",
    "    return results\n",
    "\n",
    "def restartClientFunction():\n",
    "    client = Client('192.168.3.100:8786',timeout=3)\n",
    "    client.restart()\n",
    "    client.close()\n",
    "\n",
    "def ts_cv_split (dataset):\n",
    "    #rmse = []\n",
    "\n",
    "    both_ = []\n",
    "    #train_ = []\n",
    "    #test_ = []\n",
    "    for train_index, test_index in tscv.split(outer_dataset.index):\n",
    "        #train_.append(train_index)\n",
    "        #test_.append(test_index)\n",
    "        both_.append([train_index,test_index])    \n",
    "    return(both_)\n",
    "\n",
    "def return_ts_cv_data (indexes):\n",
    "    dataset=outer_dataset\n",
    "    #print(indexes[0])\n",
    "    return([dataset.iloc[indexes[0]],dataset.iloc[indexes[1]]])\n",
    "\n",
    "def cv_pcor_check (npa_):\n",
    "    \n",
    "    #data = npa_\n",
    "    n = npa_[2]\n",
    "    New_Names_testing = list(np.array(New_Names)[(np.array(New_Names)!=n)])\n",
    "    #print(npa_[0])\n",
    "    \n",
    "    #dataset= outer_dataset\n",
    "    #train_index = npa_[0]\n",
    "    #print(train_index)\n",
    "    #test_index = npa_[1]\n",
    "    \n",
    "    #I don't need it to do training/test splits, but I had advanced ideas that would apply linear models to a test partition and go with the best error reduction... \n",
    "    # but partial correlations are just that except they don't take into consideration training/test partitions\n",
    "\n",
    "    #target.iloc[training].iloc[train_index]\n",
    "    subset_train = npa_[0]#dataset.iloc[train_index]\n",
    "    train_index = subset_train.index\n",
    "    subset_train = subset_train.dropna()\n",
    "    subset_test = npa_[1]#dataset.iloc[test_index]\n",
    "    #return(subset_test)\n",
    "    test_index = subset_test.index\n",
    "    subset_test = subset_test.dropna()\n",
    "    \n",
    "    y_reg_train_no_x = LinearRegression().fit(subset_train[New_Names_testing], subset_train['target'])\n",
    "    y_fore_no_x = y_reg_train_no_x.predict(subset_test[New_Names_testing])\n",
    "    y_resid_no_x = y_fore_no_x.ravel()-subset_test['target']\n",
    "    \n",
    "    x_reg_train_no_x = LinearRegression().fit(subset_train[New_Names_testing], subset_train[n])\n",
    "    x_fore_no_x = x_reg_train_no_x.predict(subset_test[New_Names_testing])\n",
    "    x_resid_no_x = x_fore_no_x.ravel()-subset_test[n]\n",
    "    \n",
    "    cor_resid = pd.concat([pd.DataFrame(y_resid_no_x),pd.DataFrame(x_resid_no_x)],axis=1).corr()\n",
    "    #model_name = ols(formula_from_cols(subset, 'target'),data=data_final_dask_w_y[subset.columns].compute().iloc[train_index]).fit()\n",
    "    #print(model_name.summary())\n",
    "\n",
    "    #skip y and states\n",
    "    #set_ = subset.loc[:, ~subset.columns.isin([target])].columns.tolist()\n",
    "    \n",
    "    c_value = np.array(cor_resid).ravel()[1]\n",
    "    \n",
    "    return(c_value)\n",
    "\n",
    "def pvalues(n):\n",
    "    #n = New_Names[0]\n",
    "    New_Names_testing = list(np.array(New_Names)[(np.array(New_Names)!=n)])\n",
    "\n",
    "    p_values = pd.DataFrame()\n",
    "    #inner_c_values = []\n",
    "\n",
    "    #outer_dataset is derived from trainings\n",
    "    indexes = ts_cv_split(outer_dataset)\n",
    "    \n",
    "    data = clientFunction(return_ts_cv_data,indexes)\n",
    "    #print(data)\n",
    "    #print(data)\n",
    "    new_data = []\n",
    "    \n",
    "    for d in data:\n",
    "        #print(d[0])\n",
    "        #print(d[1])\n",
    "        #print(n)\n",
    "        new_data.append([d[0],d[1],n])\n",
    "    #inner_c_values = []\n",
    "    #print(new_data[0][0])\n",
    "    #print(cv_pcor_check(new_data[0]))\n",
    "    #here test against holdout data is done\n",
    "    inner_c_values = clientFunction(cv_pcor_check,new_data)\n",
    "    print(inner_c_values)\n",
    "    #loops\n",
    "    \n",
    "    #for d in data:\n",
    "        #inner_c_values.append(cv_pcor_check(d))\n",
    "\n",
    "    n_ = len(indexes[1][0]) \n",
    "\n",
    "    dist = scipy.stats.beta(n_/2 - 1, n_/2 - 1, loc=-1, scale=2)\n",
    "    p_value = 2*dist.cdf(-abs(np.mean(inner_c_values)))\n",
    "    temp = pd.DataFrame([chosen,n,p_value]).T\n",
    "    temp.columns = ['target','test','p']\n",
    "\n",
    "    if(np.isnan(p_value)):\n",
    "        #print(n)\n",
    "        #print(inner_c_values)\n",
    "        p_value = 0\n",
    "    #p_values = pd.concat([p_values,temp],axis=0)\n",
    "    return(p_value)\n",
    "\n",
    "def y_subset(df):\n",
    "    \n",
    "    X = list ()\n",
    "    \n",
    "    for var_pos in range(0,len(df.columns)):\n",
    "        variables=df.columns\n",
    "        target=variables[var_pos]\n",
    "        #print(target)\n",
    "        #print(variables.isin([target]))\n",
    "        temp = pd.concat([pd.DataFrame(df[target]),df.loc[:, ~df.columns.isin([target])]],axis=1)\n",
    "        #print(temp)\n",
    "        X.append(temp)\n",
    "    return(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8ef083-5f57-410b-8c5a-0b476293b4e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f6b876-1953-4937-bc06-3c05123d53b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e958a51-c9e2-4aa4-a56a-fa7b21fef44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit(n_splits = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33a75df-bb32-49a6-9fb6-1275c50118bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8522fc-a4da-4a03-a113-ab647a07910d",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pd.read_csv(\"all_data.csv\",index_col=0)\n",
    "raw.index = pd.to_datetime(raw.index)\n",
    "\n",
    "#fillna(method='bfill')\n",
    "raw_int = raw.interpolate(method='time').dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f46ab1-885b-445b-aad6-d4216c46acd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e381b0b6-e08c-4b8e-ab95-f18116a375e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = train_test_split(raw_int.index, test_size=.5, random_state=0, shuffle=False)\n",
    "\n",
    "test_sets = []\n",
    "\n",
    "for i in indexes:\n",
    "    test_sets.append(raw_int.index.difference(i))\n",
    "    \n",
    "training = indexes[0]\n",
    "testing = indexes[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9539ddc5-f4d2-405a-b6b5-463192afa593",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delta = (raw_int-raw_int.shift()).dropna()\n",
    "#raw_delta = (raw_int - raw_int.apply(lag,0)).dropna()\n",
    "#raw_delta.head()\n",
    "\n",
    "#raw_delta.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cd80bd-da12-4330-bb5d-fa81b6a9eac2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215c3655-065a-4b5f-bf22-1f20181c718e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#for i in range(0,len(raw_int.columns)):\n",
    "        \n",
    "#np.max(sndif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb95b5fa-44cc-4ea7-adb1-7f21bd8b1f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_int.diff().dropna().apply(pmdarima.arima.nsdiffs(m=4))\n",
    "\n",
    "sndif = []\n",
    "\n",
    "season = 4\n",
    "maxn = season\n",
    "\n",
    "npa = []\n",
    "\n",
    "for s in range(0,len(raw_int.columns)):\n",
    "    npa.append([raw_int.columns[s],maxn,training])\n",
    "    \n",
    "sndif = clientFunction(sndif_,npa)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2129f972-961c-4202-aae0-d6d9b3445422",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sndif_(npa[0])\n",
    "#sndif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee439e23-992e-4dce-8cad-524c2f95eed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndif = []\n",
    "\n",
    "npa = []\n",
    "\n",
    "for s in range(0,len(raw_int.columns)):\n",
    "    npa.append([raw_int.columns[s],training])\n",
    "    \n",
    "ndif = clientFunction(ndif_,npa)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8556689a-0f4e-4111-a5fa-30da2d1985da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52aaa60d-8804-4476-af02-bc5813637247",
   "metadata": {},
   "outputs": [],
   "source": [
    "#doesn't preserve na's...\n",
    "#len(pmdarima.utils.diff(temp,1,1).ravel())\n",
    "\n",
    "deseasoned = pd.DataFrame()\n",
    "for i in range(0,len(raw_int.columns)):\n",
    "    if(sndif[i]*season == 0):\n",
    "        temp = raw_int.iloc[:,[i]]\n",
    "    else:\n",
    "        temp = raw_int.iloc[:,[i]]\n",
    "        if(sndif[i]>0):\n",
    "            for d in range(0,sndif[i]):\n",
    "                temp = pd.DataFrame(temp.values.ravel()-lagpad(temp.values.ravel(),1*season)).set_index(temp.index)\n",
    "                temp.columns = raw_int.iloc[:,[i]].columns\n",
    "    deseasoned = pd.concat([deseasoned,temp],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f2455f-2517-4844-a92d-f0870c4ebfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for d in deseasoned.columns:\n",
    "    #print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3876d330-9fc9-484b-8be9-562cebe91ef3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211bbabb-94ff-47b2-8ee3-56fe70ff46c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "deseasoned_differenced = pd.DataFrame()\n",
    "\n",
    "for i in range(0,len(raw_int.columns)):\n",
    "    temp_ = deseasoned.iloc[:,[i]]\n",
    "    colnames = temp_.columns\n",
    "    if ndif[i]>0:\n",
    "        #print(ndif[i])\n",
    "        for d in range(0,ndif[i]):\n",
    "            #print(d)\n",
    "            #\n",
    "            #print(temp_.columns)\n",
    "            #temp_ = pd.DataFrame(temp_.values.ravel()-lagpad(temp_.values.ravel(),1)).set_index(temp_.index)\n",
    "            #temp_.columns = colnames\n",
    "            temp = temp_.diff()\n",
    "    temp.columns = temp_.columns\n",
    "    deseasoned_differenced = pd.concat([deseasoned_differenced,temp],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0674d6d-026c-4d0f-bfb5-6d076165a3c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6874c8-c189-422d-8743-6d398590d6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for d in deseasoned_differenced.columns:\n",
    "    #print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f309a8f4-665b-473b-b35f-ceb1cbcb7246",
   "metadata": {},
   "outputs": [],
   "source": [
    "deseasoned_differenced.interpolate(method='time').isna().sum().sum()\n",
    "#.fillna(method='bfill')\n",
    "#raw_int = raw.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a7c307-5b10-4e14-936c-3665c4579e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "deseasoned_differenced.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfebf96-1bd9-41ed-855d-7ada55f8682f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for d in deseasoned_differenced.columns:\n",
    "    #print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445377f0-481e-4147-8396-662f98b29fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://machinelearningmastery.com/time-series-data-stationary-python/\n",
    "cleaned = deseasoned_differenced.interpolate(method='time')#.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bc1b23-ea20-462b-a7eb-187202d5254a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(cleaned.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3c28a1-d38f-4a76-a0e4-6867ccd7fdf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca41a320-0a65-4655-a883-7979a533bba8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fb0918-211c-4e91-96d2-ed65662ba25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many are stationary?\n",
    "%matplotlib inline\n",
    "pd.DataFrame(cleaned.dropna().apply(adfuller).iloc[1,]).iloc[:,0].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44357db8-49cd-4001-b834-7708f0b0a4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for d in cleaned.columns:\n",
    "    #print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a64a2b4-4999-44c7-a16a-022499effcce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e6f0e4-00b9-4d1f-a0d5-c96148ba8280",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.cumsum(x_names=='BOGZ1FL105015105Q')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10040ac7-710f-4822-ac65-dd624ea1cd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(cleaned.iloc[:,raw.columns=='BACDINA066MNFRBNY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170f5bde-8c64-44d0-97c6-955504a29dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for c in cleaned.columns:\n",
    "    #print(y_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fb03f9-0620-4717-aa8b-d42d540bac92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3842ba-3b4a-4abb-ac5d-2a210de4d076",
   "metadata": {},
   "outputs": [],
   "source": [
    "ccf_max_lag = 4\n",
    "\n",
    "ccf_ = []\n",
    "\n",
    "npa = []\n",
    "\n",
    "#chosen = cleaned.columns[random.randint(0,len(cleaned.columns)-1)]\n",
    "chosen = 'MSPUS'\n",
    "chosen = 'LXXRCSA'\n",
    "print(chosen)\n",
    "y_name = cleaned.columns[cleaned.columns==chosen].values[0]\n",
    "#x_names = cleaned.columns[(cleaned.columns!=cleaned.columns[0])]\n",
    "x_names = cleaned.columns\n",
    "\n",
    "for s in range(0,len(cleaned.columns)):\n",
    "    #y_name = y_name_\n",
    "    x_name = x_names[s]\n",
    "    #print(x_name)\n",
    "    npa.append([y_name,x_name,training])\n",
    "    \n",
    "ccf_ = clientFunction(ret_ccf,npa)\n",
    "\n",
    "y = np.array(cleaned.iloc[:,cleaned.columns==y_name]).ravel()\n",
    "x = y\n",
    "#last one is for comparing with itsel to ensure 0 lag ccf is 1\n",
    "ccf_.append([y_name,y_name,crosscorrelation(x,y, ccf_max_lag, mode='corr')])\n",
    "#ccf_.append([np.array([y_name,y_name]).reshape(2,1),crosscorrelation(x,y, 4, mode='corr')])#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4d08d6-1f0c-4447-be2d-a3a6b91a1fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(cleaned[chosen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9e6ab6-e639-4dce-8db4-e29a2d729119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4440f99-f158-43d1-9108-6967fc0122aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "range_ = [*range(-ccf_max_lag,ccf_max_lag+1)].copy()\n",
    "\n",
    "ccf_scores = pd.DataFrame()\n",
    "\n",
    "for c in ccf_:\n",
    "    #print(c)\n",
    "    y = c[0]\n",
    "    x = c[1]\n",
    "    #print(x)\n",
    "    ar_ = pd.DataFrame(c[2])\n",
    "    #print(ar_)\n",
    "    ar_.index = range_\n",
    "    ar_.columns = [x]\n",
    "    #abs(ar_)\n",
    "    ccf_scores = pd.concat([ccf_scores,ar_],axis=1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df77cb1-9235-4db8-ab3e-826f65c44330",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0058905-36bc-476c-b52a-b8fde036eb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(ccf_scores.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96543fec-bd07-4bbf-b3d0-b1431bcba01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#derive optimally lagged dataset\n",
    "\n",
    "data_final = pd.DataFrame()\n",
    "\n",
    "best_lags = []\n",
    "#don't want the last one\n",
    "for c in ccf_scores.columns[:-1]:\n",
    "    #print(c)\n",
    "    temp = ccf_scores[ccf_scores.index>0][c]\n",
    "    bl = ccf_scores.index[ccf_scores.index>0][np.argmax(abs(temp))]\n",
    "    best_lags.append(bl)\n",
    "    data = pd.DataFrame(lagpad(cleaned[c],bl))\n",
    "    data.index = cleaned[c].index\n",
    "    data.columns = [c]\n",
    "    data_final = pd.concat([data_final,data],axis=1)\n",
    "    \n",
    "temp = pd.DataFrame(cleaned[chosen])\n",
    "temp.columns = ['target']\n",
    "data_final = pd.concat([temp,data_final],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b577961c-4929-4071-8f22-97381fead1e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224de44f-3fc4-4630-85ac-622a7d8d3c2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a35a30-937e-4751-836a-8cf447037e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_final_dask = dd.from_pandas(data_final,npartitions=128)\n",
    "#data_final_dask_w_y = dd.concat([cleaned[[y_name]],data_final_dask.compute()],axis=1)\n",
    "#names_ = ['target']\n",
    "#names_.extend(cleaned.columns)\n",
    "#data_final_dask_w_y.columns = names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cc2b44-72f9-46b4-b3fc-a10c46fd0c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_final_dask.apply(np.cumsum,axis=1).compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce954cbf-4eca-4d46-a2af-afa25e3e88f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac13186e-5615-44b8-8593-067501c7fcb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea2c216-e4cc-4ce5-aead-bcb283cd801c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(data_final.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d33c54e-9bd1-4012-b11a-6a28f1cc8198",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(raw_int[chosen])\n",
    "#plt.plot(cleaned[chosen])\n",
    "#plt.plot(data_final_dask_w_y[chosen].compute())\n",
    "#plt.plot(data_final_dask_w_y[['target']].compute().loc[training])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2259a1-3d45-43b5-9f1a-4715ed7adf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.sort(data_final_dask_w_y.compute().isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a92423-63d4-4d38-85ed-dc9e910eef5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17484b9-be53-4ab5-ae8c-1b43b45e4ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "zca1_subset = working_set[subset.columns.difference(['target',max_pname])].dropna()\n",
    "zca1 = zca.fit(zca1_subset)\n",
    "zca1_df = pd.DataFrame(zca1.transform(zca1_subset))\n",
    "zca1_df.columns = zca1_subset.columns\n",
    "zca1_df.index = zca1_subset.index\n",
    "\n",
    "zca1_subset = working_set[subset.columns.difference(['target',max_pname])].dropna()\n",
    "zca1 = zca.fit(zca1_subset)\n",
    "zca1_df = pd.DataFrame(zca1.transform(zca1_subset))\n",
    "zca1_df.columns = zca1_subset.columns\n",
    "zca1_df.index = zca1_subset.index\n",
    "#pd.concat([target,zca1],axis=1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cd5876-0407-4d06-adc3-bb6e5d3ad84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620eabbd-bca0-4db4-a177-911270eeb318",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = y_subset(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693debdc-01f1-4e2e-9a79-8e80f1b885eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b784a9c4-e35a-45f8-9cc6-779a645bd20c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d705c499-58c2-4e07-a2b5-9a32d69e67ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_f_s = regress(data_final)\n",
    "#.compute().loc[training],training,testing\n",
    "temp_train = data_final.loc[training].dropna()\n",
    "temp_test = data_final.loc[testing].dropna()\n",
    "\n",
    "target = temp_train.columns[0]\n",
    "#.fit is X, y format\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    fitted = s_f_s.fit(temp_train.loc[:, ~temp_train.columns.isin(['target'])], pd.DataFrame(temp_train['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a6ddca-6ce8-43af-a504-6b89ab6a479d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19ed9f7-ee6e-407c-9206-8e3a04605112",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plot_sfs(fitted.get_metric_dict(), kind='std_err')\n",
    "plt.title('Sequential Forward Selection (w. StdErr)')\n",
    "plt.savefig(str(target)+'.png', dpi=300, format='png', bbox_inches='tight')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96987af-87b6-4dc1-9764-bdf1c85d6e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7db84a3-b092-4b34-a833-dc4061cd68b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_table = pd.DataFrame(fitted.get_metric_dict()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6034af74-a848-424b-8143-4a31abe3cf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413f0cea-63fd-430f-b4e7-b1be3dfff54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "winners = metric_table[metric_table['avg_score']<(np.std(metric_table['avg_score'])+np.min(metric_table['avg_score']))].tail(1)['feature_names'].index\n",
    "winners_ = np.asarray(fitted.get_metric_dict()[winners[0]]['feature_names'])\n",
    "\n",
    "knee_last = np.min(np.where(np.round([*metric_table['avg_score']],6)==np.round(np.max([*metric_table['avg_score']]),6)))\n",
    "\n",
    "#elbow method beats the other method\n",
    "temp_df = findknee(np.array(metric_table['avg_score'][0:(knee_last+1)]))\n",
    "winners_ = np.asarray(metric_table.iloc[np.min(np.where([temp_df==np.max(temp_df)])[1])]['feature_names'])\n",
    "#winners_ = np.asarray(metric_table.iloc[10]['feature_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d72e16-2442-4265-af9d-9eb7010d47c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce01763-dd51-436d-9966-a7d84d580005",
   "metadata": {},
   "outputs": [],
   "source": [
    "winners_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ef0162-f7ce-44a1-996c-8d1bc64690d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c8050c-e9cf-4c1b-ac6b-da5a86ea71b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(metric_table['avg_score'][0:(knee_last+1)]))\n",
    "\n",
    "metric_table.loc[metric_table['avg_score']<=np.std(abs(metric_table['avg_score'][0:(knee_last+1)]))+np.min(abs(metric_table['avg_score'][0:(knee_last+1)]))+np.min((metric_table['avg_score'][0:(knee_last+1)]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b3c525-303a-4ba7-8ed9-286852a10e89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd757a6a-8c83-473d-9f63-f3d3f50fb6b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fd52b1-00ad-4862-81f4-79bf6dbe11c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(metric_table['avg_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf87fc6c-2463-43b6-8417-3a1e6d1b9628",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2430ee8f-f974-47da-9f6e-d9193f067d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sm.add_constant(temp_train[winners_])\n",
    "results = sm.OLS(temp_train['target'],X).fit()\n",
    "print(results.summary())\n",
    "\n",
    "inverses = []\n",
    "\n",
    "print(chosen)\n",
    "\n",
    "value = np.where(raw_int.columns==chosen)[0][0]\n",
    "\n",
    "print('sndif:', sndif[value])\n",
    "print('ndif:', ndif[value])\n",
    "inverses.append([chosen,sndif[value],ndif[value]])\n",
    "\n",
    "forecast = results.predict(temp_test[np.concatenate([['target'],winners_])])\n",
    "\n",
    "print(MAPE(temp_test['target'],forecast))\n",
    "\n",
    "index_ = []\n",
    "\n",
    "for w in winners_:\n",
    "    print(w)\n",
    "    value = np.where(raw_int.columns==w)[0][0]\n",
    "    index_.append(value)\n",
    "    inverses.append([w,sndif[value],ndif[value]])\n",
    "    print('sndif:', sndif[value])\n",
    "    print('ndif:', ndif[value])\n",
    "\n",
    "%matplotlib inline\n",
    "corrMatrix = pd.concat([temp_train['target'],temp_train[winners_]],axis=1).pcorr().sort_values(kind=\"quicksort\", by='target', ascending=False,key=abs)\n",
    "sn.heatmap(corrMatrix, annot=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9a72a8-c280-4e6d-8b3d-c8d98ac73219",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f623afd-39d9-44bf-b185-f0e0071fe26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonal = inverses[0][1]\n",
    "print(seasonal)\n",
    "nonseasonal = inverses[0][2]\n",
    "print(nonseasonal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9545641-8a05-4190-920f-f8eb509bd30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def undiff(data, seasonal, nonseasonal, xi):\n",
    "    \n",
    "    #for undiff to work properly, initial values need to be provided\n",
    "    \n",
    "    \n",
    "    #nonseasonal\n",
    "    if(nonseasonal!=0 and seasonal==0):\n",
    "        temp = np.concatenate([np.array(xi),np.array(data)])\n",
    "        temp_ = diff_inv(temp,1,nonseasonal)\n",
    "        return(temp_[-len(temp):])\n",
    "        \n",
    "    #seasonal\n",
    "    if(seasonal!=0 and nonseasonal == 0):\n",
    "        temp = np.concatenate([np.array(xi),np.array(data)])\n",
    "        temp_ = diff_inv(temp,season,1)\n",
    "        return(temp_[-len(temp):])\n",
    "    \n",
    "    #both\n",
    "    #if(seasonal==1 & nonseasonal == 1):\n",
    "        #temp_ = diff_inv(diff_inv(temp,season,1),1,nonseasonal)\n",
    "    if(seasonal==1 and nonseasonal == 1):\n",
    "        temp_ = data\n",
    "\n",
    "        initial_seasonal_delta = xi.iloc[season]-xi.iloc[0]\n",
    "        s_diffed_cat = np.concatenate([[np.array(initial_seasonal_delta)],np.array(temp_.dropna())])\n",
    "\n",
    "        ns_undiffed = diff_inv(s_diffed_cat,1,1)[-len(temp_.dropna()):]\n",
    "\n",
    "        ns_diffed_cat = np.concatenate([np.array(xi[1:(season+1)]),np.array(ns_undiffed)])\n",
    "        s_undiffed = diff_inv(ns_diffed_cat,season,1)[-len(ns_undiffed):]\n",
    "\n",
    "        return(s_undiffed)\n",
    "        \n",
    "def difference(data, seasonal, nonseasonal):\n",
    "\n",
    "    if seasonal > 0:\n",
    "        return(data.diff(periods=season).diff(nonseasonal))\n",
    "    elif season > 0:\n",
    "        return(data.diff(nonseasonal))\n",
    "    else:\n",
    "        return(data)\n",
    "    \n",
    "#non seasonal\n",
    "#undiff(temp_test['target'].dropna(), 0, nonseasonal,[raw_int[chosen].loc[temp_train['target'].index[-1]]])\n",
    "\n",
    "#seasonal\n",
    "#undiff(raw_int[chosen].diff(periods=4).dropna(),1,0,raw_int[chosen][0:season])\n",
    "\n",
    "#non seasonal and seasonal\n",
    "#undiff(raw_int[chosen].diff(periods=4).diff().dropna(),seasonal,nonseasonal,raw_int[chosen][0:season+nonseasonal])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f0aee0-9b5b-4908-9421-91c262805cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d4be9f-80ba-4dee-b7f1-e87393730b63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "id": "35167fce-eadb-403b-a708-12be5068a86b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "2008-03-31    247.671510\n",
       "2008-06-30    229.624413\n",
       "2008-09-30    216.498708\n",
       "2008-12-31    204.519057\n",
       "2009-03-31    191.596232\n",
       "Name: LXXRCSA, dtype: float64"
      ]
     },
     "execution_count": 873,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4393340a-452f-4831-a71c-9fc969f2e816",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "undiff(raw_int[chosen].diff(periods=4).diff().dropna(),1,1,raw_int[chosen][0:season+nonseasonal])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b13d59-38d1-4fb2-946d-06db1bad2baa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207cae6d-9c09-4342-a7f2-dc7e59455a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = raw_int[chosen].diff(periods=4)\n",
    "temp = np.concatenate([np.array(xi),np.array(data)])\n",
    "diff_inv(diffed.dropna(),season,xi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900eac71-84c7-4e54-989b-2f78c99aace5",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_int[chosen].diff(periods=4).diff().dropna(),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3320bca5-683f-4ca5-a842-e4fef3c779d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "diffed = difference(raw_int[chosen],seasonal, season)\n",
    "\n",
    "undiffed_test = undiff(temp_test['target'], seasonal, nonseasonal)\n",
    "undiffed_train = undiff(temp_train['target'], seasonal, nonseasonal)\n",
    "undiffed_forecast = undiff(forecast, seasonal, nonseasonal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71755a17-dfa2-479d-a3f9-d91134239c45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508fe348-072f-4814-a7c3-af7f24cf7f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "newindex = []\n",
    "\n",
    "for i in range(0,len(temp2_.index)):\n",
    "    newindex.append(temp2_.index[i])\n",
    "\n",
    "for i in range(0,inverses[0][2]):\n",
    "    newindex.append(last_day_of_month(temp2_.index[-1] + pd.DateOffset(90*i)))\n",
    "    \n",
    "len(newindex)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76b9bbe-845d-45fa-8d9a-3474936ef17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_int[chosen].iloc[position_og_raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79efe80a-2038-4e51-bc08-3f01ab9cd2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_int[chosen].iloc[position_og_raw]+undiffed_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505d2702-90a7-4235-bd61-9897f4640935",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chosen)\n",
    "\n",
    "position_og_raw = (np.where(raw_int.index==temp_train['target'].index[0])[0][0]-1)\n",
    "print(temp_train['target'].index[0])\n",
    "print(position_og_raw)\n",
    "print(raw_int[chosen].iloc[position_og_raw])\n",
    "\n",
    "position_fore_og_raw = (np.where(raw_int.index==temp_test['target'].index[0])[0][0]-1)\n",
    "print(temp_test['target'].index[0])\n",
    "print(position_fore_og_raw)\n",
    "print(raw_int[chosen].iloc[position_fore_og_raw])\n",
    "\n",
    "\n",
    "#.set_index(pd.DatetimeIndex(newindex))+temp2_['target']\n",
    "#undifferenced_forecast_raw.columns = ['target']\n",
    "\n",
    "#pd.DataFrame(temp2_['target'] + raw_int[chosen].iloc[position_fore_og_raw]).set_index(raw_int.index[position_fore_og_raw:(position_fore_og_raw+len(temp2_))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce05346-9625-4507-b1d1-ce185ec4c3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_int[chosen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27a191c-dd5c-4af9-82f6-b56d16737e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f4255b-d623-4d98-9af5-0973f938dffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pd.concat([undifferenced_forecast_raw,raw_int[chosen].loc[undifferenced_forecast_raw.index[0:-nonseasonal+seasonal*4]]],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ada8bae-5194-4301-9cab-f75007eed0e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c7720a-d946-427e-9a0b-24e8566f2a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#n = New_Names[3]\n",
    "#New_Names==n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37831a7d-5759-4f4c-8eef-5aca739e9f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#New_Names = (data_final_dask_w_y.columns.difference(['target']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51702ec-ba0b-48f1-bdc5-6f22cae90e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "\n",
    "future = client.map(regress, X)\n",
    "\n",
    "results = []\n",
    "best = -1\n",
    "for f in as_completed(future):\n",
    "    results.append(f.result())\n",
    "    \n",
    "def y_subset(df):\n",
    "    \n",
    "    X = list ()\n",
    "    \n",
    "    for var_pos in range(0,len(df.columns)):\n",
    "        variables=df.columns\n",
    "        target=variables[var_pos]\n",
    "        #print(target)\n",
    "        #print(variables.isin([target]))\n",
    "        temp = pd.concat([pd.DataFrame(df[target]),df_.loc[:, ~df.columns.isin([target])]],axis=1)\n",
    "        #print(temp)\n",
    "        X.append(temp)\n",
    "    return(X)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f0d63e-85ca-449b-8390-aeeecb71313f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "scaler = StandardScaler()\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits = 5)\n",
    "\n",
    "#scaler.fit(np.array(data_final_dask_w_y[['target']].compute().loc[training]).reshape(-1, 1))\n",
    "\n",
    "New_Names = list(data_final_dask_w_y.columns.difference(['target']))\n",
    "\n",
    "outer_dataset = data_final_dask_w_y.compute().loc[training].dropna()\n",
    "target = outer_dataset[['target']]\n",
    "\n",
    "subset = pd.concat([target,outer_dataset[New_Names]],axis=1)\n",
    "\n",
    "num_folds = 2\n",
    "#kfold = KFold(n_splits=num_folds, shuffle=False)\n",
    "#train, test = kfold.get_n_splits(outer_dataset.index)\n",
    "\n",
    "p_threshold = .05\n",
    "\n",
    "iteration = 0\n",
    "max_pvalue = 1\n",
    "\n",
    "while(max_pvalue>=.05):\n",
    "#\n",
    "    print(chosen)\n",
    "    \n",
    "    print(New_Names)\n",
    "    \n",
    "    n_p_values = pd.DataFrame()\n",
    "\n",
    "    p_values = []\n",
    "    \n",
    "    #parallelize here (x16)\n",
    "    for n in New_Names:\n",
    "        #print(n)\n",
    "        New_Names_testing = list(np.array(New_Names)[(np.array(New_Names)!=n)])\n",
    "        p_values.append(pvalues(n))\n",
    "        \n",
    "    p_values_df = pd.DataFrame(p_values,index=New_Names)\n",
    "    print(p_values_df)\n",
    "\n",
    "    max_pname = New_Names[np.argmax(p_values_df)]\n",
    "    max_pvalue = p_values[np.argmax(p_values_df)]\n",
    "\n",
    "    #n_p_values = pd.concat([n_p_values,p_values],axis=0)\n",
    "    #print(n_p_values)\n",
    "\n",
    "    if (max_pvalue > .05):\n",
    "        print([max_pname, max_pvalue])\n",
    "        #New_Names.remove(max_pname)\n",
    "        #New_Names_testing = list(np.array(New_Names_testing)[(np.array(New_Names_testing)!=max_pname)])\n",
    "        New_Names = list(np.array(New_Names)[(np.array(New_Names)!=max_pname)])\n",
    "        temp = ['target']\n",
    "        temp.extend(New_Names)\n",
    "        subset = subset[temp]\n",
    "    print()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40460a2a-3b85-4e98-a95e-792e089b57ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "restartClientFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c159e1b-117f-48fc-87f8-b402c6310ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_temp = data_final_dask_w_y.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f421c1-f54c-4de0-a658-f83efe327bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset[New_Names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340c1eec-b1c3-4ec2-9c10-4cdd03192cf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7523b67a-6bd8-46a1-91e7-345d9fc2f12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "includes = []\n",
    "for c in subset.columns:\n",
    "    index = np.argwhere(data_final_dask_w_y.columns==c)[0][0]\n",
    "    includes.append(index)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db12096a-43ea-4eff-aee9-1c47bb6230cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b53efc9-e332-4828-b7b3-a8f3cf70118b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reg = train(data_final_dask_w_y[subset.columns].compute().loc[training].dropna())\n",
    "#subset.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572f337a-3486-4c7e-b469-74ce5d877360",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reg = regress(data_final_dask_w_y[subset.columns].compute().dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8468939-ef88-4d8d-888f-fcdcf4d6e56f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
