{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c842ae64-4502-42d2-aeab-22d1cd96e13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install clustergram pandas_profiling scipy sklearn statsmodels IPython dtale matplotlib rpy2 seaborn shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf49edd-ccdb-4a5a-9ede-a9b3c081ceb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#put in ~/.bashrc\n",
    "#LD_PRELOAD=\"/mnt/distvol/R/4.1.2/lib64/R/lib/LibR.so\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab96046e-bb6b-420f-b6b1-9524e60fe678",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from fracdiff import fdiff\n",
    "#import urbangrammar-graphics as ugg\n",
    "%matplotlib inline\n",
    "import os\n",
    "from clustergram import Clustergram\n",
    "from concurrent.futures import wait, ALL_COMPLETED\n",
    "from dask.distributed import Client\n",
    "from dask.distributed import as_completed\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as sfs\n",
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from numpy import absolute\n",
    "from numpy import arange\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from pandas_profiling import ProfileReport\n",
    "#from rpy2.robjects import pandas2ri\n",
    "from pmdarima.utils import diff_inv\n",
    "from rpy2.robjects.conversion import localconverter\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.rinterface_lib import openrlib\n",
    "from scipy import stats\n",
    "from scipy.cluster.vq import vq\n",
    "from scipy.spatial.distance import cdist, pdist\n",
    "from scipy.special import boxcox, inv_boxcox\n",
    "from scipy.stats import f\n",
    "from sklearn import preprocessing\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import *\n",
    "#from sklearn.preprocessing import PowerTransformer\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn.preprocessing import scale\n",
    "from sklearn.utils import as_float_array\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.stats.outliers_influence import OLSInfluence\n",
    "import statsmodels.api as sm\n",
    "import IPython\n",
    "import concurrent.futures\n",
    "import dask.dataframe as dd\n",
    "import datetime\n",
    "import dtale\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pingouin as pg\n",
    "import pmdarima\n",
    "import pycorrelate\n",
    "import random\n",
    "import re\n",
    "import rpy2\n",
    "import rpy2.robjects as ro\n",
    "import rpy2.situation\n",
    "import scipy\n",
    "import seaborn as sn\n",
    "import shap\n",
    "import sklearn\n",
    "import sklearn.linear_model\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tools\n",
    "import sys\n",
    "import time\n",
    "if not sys.warnoptions:\n",
    "\timport warnings\n",
    "\twarnings.simplefilter(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693a8c73-5c94-4d9a-bf1b-9ffbe6836975",
   "metadata": {},
   "outputs": [],
   "source": [
    "#c = get_config()\n",
    "libpath = os.environ.get('LD_LIBRARY_PATH', '')\n",
    "os.environ['LD_LIBRARY_PATH'] = (\n",
    "    rpy2.situation.r_ld_library_path_from_subprocess(openrlib.R_HOME) +\n",
    "    libpath\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c465a19f-5039-4627-8b7b-23d0b18f6017",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regress (dd_df):\n",
    "    \n",
    "    lr = LinearRegression()\n",
    "    print(\"y needs to be named 'target', regress only uses the variable names, it doesn't use index's.  You apply that using .fit on this functions return\")\n",
    "    #dd_df = data_final\n",
    "    variables=dd_df.columns\n",
    "    target = variables[0]\n",
    "    te = pd.DataFrame(dd_df['target']) \n",
    "    te.index = ([*dd_df.index])\n",
    "    #te.columns = ['target']\n",
    "    col_names = variables[~variables.isin(['target'])].ravel()\n",
    "\n",
    "    s_f_s = sfs(lr, \n",
    "              k_features=np.int(len(col_names)*.05), \n",
    "              forward=True, \n",
    "              floating=True, \n",
    "              scoring='neg_mean_absolute_percentage_error',\n",
    "              n_jobs=-1,\n",
    "              cv=5)\n",
    "\n",
    "    return (s_f_s)\n",
    "\n",
    "def last_day_of_month(date):\n",
    "    return date.replace(day=1) + relativedelta(months=1) - relativedelta(days=1)\n",
    "\n",
    "def findknee(xdata):\n",
    "    rate_of_change=(xdata[0]-xdata[-1])/(len(xdata)-1)\n",
    "    #print(rate_of_change)\n",
    "    delta = xdata-xdata[-1]\n",
    "    deltas = []\n",
    "    deltas.append(delta[0])\n",
    "    for d in range(1,len(xdata)):\n",
    "        deltas.append(deltas[d-1]-rate_of_change)\n",
    "    #print(deltas)\n",
    "    for d in range(0,len(xdata)):\n",
    "        deltas[d]=delta[d]-deltas[d]\n",
    "    return(np.abs(deltas))\n",
    "    \n",
    "def MAPE(Y_actual,Y_Predicted):\n",
    "    mape = np.mean(np.abs((Y_actual - Y_Predicted)/Y_actual))*100\n",
    "    return mape\n",
    "\n",
    "def formula_from_cols(df, y):\n",
    "    return y + ' ~ ' + ' + '.join([col for col in df.columns if not col==y])\n",
    "\n",
    "def testNormal (x):    \n",
    "    \n",
    "    k2, p = stats.normaltest(x)\n",
    "    alpha = .001\n",
    "    #print(\"p = {:g}\".format(p))    \n",
    "    if p < alpha:  # null hypothesis: x comes from a normal distribution\n",
    "        #print(p)\n",
    "        #print(alpha)\n",
    "        print(\"The null hypothesis can be rejected\")\n",
    "        xt, _ = stats.yeojohnson(x)\n",
    "        #xt, _ = stats.boxcox(x)        \n",
    "        print(_)\n",
    "        xt = pd.DataFrame(xt)\n",
    "        \n",
    "        return _, pd.DataFrame(xt).set_index(x.index)\n",
    "    else:\n",
    "        print(\"The null hypothesis cannot be rejected\")    \n",
    "        return 1, pd.DataFrame(x)\n",
    "\n",
    "def inverse_boxcox (data, lambdas):\n",
    "    power = PowerTransformer(method='yeo-johnson')\n",
    "    power.lambdas_ = lambdas.values\n",
    "    return(power.inverse_transform([data]))\n",
    "    #return inv_boxcox(data, lambdas.values)\n",
    "    \n",
    "def transform_boxcox_l(data, l_):\n",
    "    transformed = pd.DataFrame()\n",
    "\n",
    "    for i in range(0,len(data.columns)):\n",
    "        #print(i)\n",
    "        if l_.iloc[i].values == 1:\n",
    "            inner_scale = data.iloc[:,i]            \n",
    "        else:\n",
    "            inner_scale = pd.DataFrame(stats.yeojohnson((data.iloc[:,i]), lmbda=l_.iloc[i].values))\n",
    "            \n",
    "        inner_scale.index = data.index\n",
    "        transformed = pd.concat([transformed,inner_scale],axis=1)\n",
    "        \n",
    "    transformed.columns = data.columns\n",
    "    return transformed\n",
    "\n",
    "def transform_boxcox (data):\n",
    "    transformed = pd.DataFrame()\n",
    "    transformed_lambdas = pd.DataFrame()\n",
    "\n",
    "    for i in range(0,len(data.columns)):\n",
    "        l, inner_scale = testNormal(data.iloc[:,i])\n",
    "        inner_scale.set_index(data.index)\n",
    "\n",
    "        transformed_lambdas = pd.concat([transformed_lambdas,pd.DataFrame(pd.Series(l))],axis=0)\n",
    "        transformed = pd.concat([transformed,inner_scale],axis=1)\n",
    "        \n",
    "    transformed.columns = data.columns\n",
    "    return transformed, transformed_lambdas\n",
    "\n",
    "def inverse_yeo(og, data_, lambda_):\n",
    "    values = []\n",
    "    for i in range(0,len(og)):\n",
    "        X = og[i]\n",
    "        X_trans = data_[i]\n",
    "        if X >= 0 and lambda_ == 0:\n",
    "            X = exp(X_trans) - 1\n",
    "        elif X >= 0 and lambda_ != 0:\n",
    "            X = (X_trans * lambda_ + 1) ** (1 / lambda_) - 1\n",
    "        elif X < 0 and lambda_ != 2:\n",
    "            X = 1 - (-(2 - lambda_) * X_trans + 1) ** (1 / (2 - lambda_))\n",
    "        elif X < 0 and lambda_ == 2:\n",
    "            X = 1 - exp(-X_trans)\n",
    "        \n",
    "        values.append(X)\n",
    "    return(pd.DataFrame(values))\n",
    "\n",
    "def revert_yeo (og, data_, lambdas):\n",
    "    reverted = pd.DataFrame()\n",
    "\n",
    "    for i in range(0,len(data_.columns)):        \n",
    "        if lambdas.iloc[i].values == 1 :\n",
    "            revert = data_.iloc[:,i]\n",
    "        else:\n",
    "            p#ower = PowerTransformer(method='yeo-johnson')\n",
    "            #power.lambdas_ = lambdas.iloc[i].values\n",
    "            #revert = pd.DataFrame(power.inverse_transform([data.iloc[:,i].values]))\n",
    "            #return inv_boxcox(data, lambdas.values)\n",
    "            revert = pd.DataFrame(inverse_yeo(og.iloc[:,i].values,data_.iloc[:,i].values, lambdas.iloc[i].values))            \n",
    "        revert.index = data_.index\n",
    "        reverted = pd.concat([reverted,revert],axis=1)\n",
    "        \n",
    "    reverted.columns = data_.columns\n",
    "    return reverted\n",
    "\n",
    "class ZCA(BaseEstimator, TransformerMixin):\n",
    "  def __init__(self, regularization=1e-5, copy=False):\n",
    "      self.regularization = regularization\n",
    "      self.copy = copy\n",
    "  def fit(self, X, y=None):\n",
    "      X = as_float_array(X, copy=self.copy)\n",
    "      self.mean_ = np.mean(X, axis=0)\n",
    "      X = X - self.mean_\n",
    "      sigma = np.dot(X.T, X) / (X.shape[0] - 1)\n",
    "      U, S, V = np.linalg.svd(sigma)\n",
    "      tmp = np.dot(U, np.diag(1 / np.sqrt(S + self.regularization)))\n",
    "      self.components_ = np.dot(tmp, U.T)\n",
    "      return self\n",
    "  def transform(self, X):\n",
    "      X_transformed = X - self.mean_\n",
    "      X_transformed = np.dot(X_transformed, self.components_.T)\n",
    "      return X_transformed\n",
    "\n",
    "def crosscorrelation(x, y, maxlag, mode='corr'):\n",
    "    \"\"\"\n",
    "    Cross correlation with a maximum number of lags.\n",
    "\n",
    "    `x` and `y` must be one-dimensional numpy arrays with the same length.\n",
    "\n",
    "    This computes the same result as\n",
    "        numpy.correlate(x, y, mode='full')[len(a)-maxlag-1:len(a)+maxlag]\n",
    "\n",
    "    The return vaue has length 2*maxlag + 1.\n",
    "    \"\"\"\n",
    "    py = np.pad(y.conj(), 2*maxlag, mode='constant')\n",
    "    T = np.lib.stride_tricks.as_strided(py[2*maxlag:], shape=(2*maxlag+1, len(y) + 2*maxlag),\n",
    "                   strides=(-py.strides[0], py.strides[0]))\n",
    "    px = np.pad(x, maxlag, mode='constant')\n",
    "    if mode == 'dot':       # get lagged dot product\n",
    "        return T.dot(px)\n",
    "    elif mode == 'corr':    # gets Pearson correlation\n",
    "        return (T.dot(px)/px.size - (T.mean(axis=1)*px.mean())) / \\\n",
    "               (np.std(T, axis=1) * np.std(px)) \n",
    "    \n",
    "def ret_ccf(npa_):\n",
    "    y_name = npa_[0]\n",
    "    x_name = npa_[1]\n",
    "    index = npa_[2]\n",
    "    \n",
    "    data = cleaned.loc[index].dropna()\n",
    "    \n",
    "    y = np.array(data.iloc[:,data.columns==y_name]).ravel()\n",
    "    \n",
    "    x = np.array(data.iloc[:,data.columns==x_name]).ravel()\n",
    "    #print(x)\n",
    "    #ccf = statsmodels.tsa.stattools.ccf(x,y)\n",
    "    ccf = crosscorrelation(x,y, ccf_max_lag, mode='corr')\n",
    "    #print(ccf)\n",
    "    return([y_name,x_name,ccf])\n",
    "\n",
    "def train(partition):\n",
    "    est = LinearRegression()\n",
    "    est.fit(partition[New_Names].values, partition['target'])\n",
    "    return est\n",
    "\n",
    "    \n",
    "    '''\n",
    "    \n",
    "def nv_diff_sets(v_of_i,dataset,f_casts):\n",
    "\n",
    "  s_=sndif_[which(colnames(raw)==var_of_int)]\n",
    "  d_=ndif_[which(colnames(raw)==var_of_int)]\n",
    "  \n",
    "  startRow = c()\n",
    "  for (r in rownames(dataset[1:d_,,drop=FALSE])):\n",
    "    startRow = c(startRow,which(rownames(raw)==r))\n",
    "  \n",
    "  data_ = c(na.omit(c(dataset[,var_of_int], f_casts)))\n",
    "  \n",
    "  if(s_==0):\n",
    "    inv_d = diffinv(data_,differences=d_,xi=raw[startRow,var_of_int])\n",
    "  else:  \n",
    "    inv_d = diffinv(diffinv(data_,differences = d_, xi=raw[startRow,var_of_int]), differences = s_,xi=raw[startRow:(startRow+season-1),var_of_int])\n",
    "    \n",
    "  return(inv_d)\n",
    "'''\n",
    "\n",
    "def lagpad(x, k):\n",
    "    length=np.full(abs(k), np.NaN)\n",
    "    #print(length)\n",
    "    #k=k-1\n",
    "    if (k>0):\n",
    "        result = np.concatenate([length,x[0:(len(x)-k)]])\n",
    "    elif (k<0):\n",
    "        result= np.concatenate([(x[abs(k):(len(x))]),length])\n",
    "    else:\n",
    "        result= x\n",
    "    return(result)\n",
    "\n",
    "def lag(data):\n",
    "    return lagpad(data,1)\n",
    "\n",
    "def sndif_(npa_):\n",
    "    index = npa_[2]\n",
    "    #print(index)\n",
    "    data = raw_int[npa_[0]].loc[index]\n",
    "    #print(data)\n",
    "    return(pmdarima.arima.nsdiffs(data.dropna(),m=npa_[1]))\n",
    "\n",
    "def ndif_(npa_):\n",
    "    index = npa_[1]\n",
    "    data = raw_int[npa_[0]].loc[index]\n",
    "\n",
    "    score = pmdarima.arima.ndiffs(data.dropna())\n",
    "    \n",
    "    if(score==0):\n",
    "        score = 1\n",
    "    return(score)\n",
    "\n",
    "def clientFunction(function_name,npa):\n",
    "    client = Client('192.168.3.100:8786',timeout=3)\n",
    "    future = client.map(function_name,npa)\n",
    "\n",
    "    results = []\n",
    "    for f in as_completed(future):\n",
    "        if(f.status==\"error\"):\n",
    "            results.append(\"error\")\n",
    "        else:\n",
    "            results.append(f.result())   \n",
    "\n",
    "    client.close()\n",
    "\n",
    "    return results\n",
    "\n",
    "def restartClientFunction():\n",
    "    client = Client('192.168.3.100:8786',timeout=3)\n",
    "    client.restart()\n",
    "    client.close()\n",
    "\n",
    "def ts_cv_split (dataset):\n",
    "    #rmse = []\n",
    "\n",
    "    both_ = []\n",
    "    #train_ = []\n",
    "    #test_ = []\n",
    "    for train_index, test_index in tscv.split(outer_dataset.index):\n",
    "        #train_.append(train_index)\n",
    "        #test_.append(test_index)\n",
    "        both_.append([train_index,test_index])    \n",
    "    return(both_)\n",
    "\n",
    "def return_ts_cv_data (indexes):\n",
    "    dataset=outer_dataset\n",
    "    #print(indexes[0])\n",
    "    return([dataset.iloc[indexes[0]],dataset.iloc[indexes[1]]])\n",
    "\n",
    "def cv_pcor_check (npa_):\n",
    "    \n",
    "    #data = npa_\n",
    "    n = npa_[2]\n",
    "    New_Names_testing = list(np.array(New_Names)[(np.array(New_Names)!=n)])\n",
    "    #print(npa_[0])\n",
    "    \n",
    "    #dataset= outer_dataset\n",
    "    #train_index = npa_[0]\n",
    "    #print(train_index)\n",
    "    #test_index = npa_[1]\n",
    "    \n",
    "    #I don't need it to do training/test splits, but I had advanced ideas that would apply linear models to a test partition and go with the best error reduction... \n",
    "    # but partial correlations are just that except they don't take into consideration training/test partitions\n",
    "\n",
    "    #target.iloc[training].iloc[train_index]\n",
    "    subset_train = npa_[0]#dataset.iloc[train_index]\n",
    "    train_index = subset_train.index\n",
    "    subset_train = subset_train.dropna()\n",
    "    subset_test = npa_[1]#dataset.iloc[test_index]\n",
    "    #return(subset_test)\n",
    "    test_index = subset_test.index\n",
    "    subset_test = subset_test.dropna()\n",
    "    \n",
    "    y_reg_train_no_x = LinearRegression().fit(subset_train[New_Names_testing], subset_train['target'])\n",
    "    y_fore_no_x = y_reg_train_no_x.predict(subset_test[New_Names_testing])\n",
    "    y_resid_no_x = y_fore_no_x.ravel()-subset_test['target']\n",
    "    \n",
    "    x_reg_train_no_x = LinearRegression().fit(subset_train[New_Names_testing], subset_train[n])\n",
    "    x_fore_no_x = x_reg_train_no_x.predict(subset_test[New_Names_testing])\n",
    "    x_resid_no_x = x_fore_no_x.ravel()-subset_test[n]\n",
    "    \n",
    "    cor_resid = pd.concat([pd.DataFrame(y_resid_no_x),pd.DataFrame(x_resid_no_x)],axis=1).corr()\n",
    "    #model_name = ols(formula_from_cols(subset, 'target'),data=data_final_dask_w_y[subset.columns].compute().iloc[train_index]).fit()\n",
    "    #print(model_name.summary())\n",
    "\n",
    "    #skip y and states\n",
    "    #set_ = subset.loc[:, ~subset.columns.isin([target])].columns.tolist()\n",
    "    \n",
    "    c_value = np.array(cor_resid).ravel()[1]\n",
    "    \n",
    "    return(c_value)\n",
    "\n",
    "#correlation p values\n",
    "def pvalues(n):\n",
    "    #n = New_Names[0]\n",
    "    New_Names_testing = list(np.array(New_Names)[(np.array(New_Names)!=n)])\n",
    "\n",
    "    p_values = pd.DataFrame()\n",
    "    #inner_c_values = []\n",
    "\n",
    "    #outer_dataset is derived from trainings\n",
    "    indexes = ts_cv_split(outer_dataset)\n",
    "    \n",
    "    data = clientFunction(return_ts_cv_data,indexes)\n",
    "    #print(data)\n",
    "    #print(data)\n",
    "    new_data = []\n",
    "    \n",
    "    for d in data:\n",
    "        #print(d[0])\n",
    "        #print(d[1])\n",
    "        #print(n)\n",
    "        new_data.append([d[0],d[1],n])\n",
    "    #inner_c_values = []\n",
    "    #print(new_data[0][0])\n",
    "    #print(cv_pcor_check(new_data[0]))\n",
    "    #here test against holdout data is done\n",
    "    inner_c_values = clientFunction(cv_pcor_check,new_data)\n",
    "    print(inner_c_values)\n",
    "    #loops\n",
    "    \n",
    "    #for d in data:\n",
    "        #inner_c_values.append(cv_pcor_check(d))\n",
    "\n",
    "    n_ = len(indexes[1][0]) \n",
    "\n",
    "    dist = scipy.stats.beta(n_/2 - 1, n_/2 - 1, loc=-1, scale=2)\n",
    "    p_value = 2*dist.cdf(-abs(np.mean(inner_c_values)))\n",
    "    temp = pd.DataFrame([chosen,n,p_value]).T\n",
    "    temp.columns = ['target','test','p']\n",
    "\n",
    "    if(np.isnan(p_value)):\n",
    "        #print(n)\n",
    "        #print(inner_c_values)\n",
    "        p_value = 0\n",
    "    #p_values = pd.concat([p_values,temp],axis=0)\n",
    "    return(p_value)\n",
    "\n",
    "def y_subset(df):\n",
    "    \n",
    "    X = list ()\n",
    "    \n",
    "    for var_pos in range(0,len(df.columns)):\n",
    "        variables=df.columns\n",
    "        target=variables[var_pos]\n",
    "        #print(target)\n",
    "        #print(variables.isin([target]))\n",
    "        temp = pd.concat([pd.DataFrame(df[target]),df.loc[:, ~df.columns.isin([target])]],axis=1)\n",
    "        #print(temp)\n",
    "        X.append(temp)\n",
    "    return(X)\n",
    "\n",
    "def undiff(data, seasonal, nonseasonal, xi):\n",
    "    \n",
    "    print(\"you have to know what xi for which use case you are going to use\")\n",
    "    \n",
    "    #nonseasonal\n",
    "    if(nonseasonal!=0 and seasonal==0):\n",
    "        temp = np.concatenate([np.array(xi),np.array(data)])\n",
    "        temp_ = diff_inv(temp,1,nonseasonal)\n",
    "        return(temp_[-len(data):])\n",
    "        \n",
    "    #seasonal\n",
    "    if(seasonal!=0 and nonseasonal == 0):\n",
    "        temp = np.concatenate([np.array(xi),np.array(data)])\n",
    "        temp_ = diff_inv(temp,season,1)\n",
    "        return(temp_[-len(data):])\n",
    "    \n",
    "    #both\n",
    "    if(seasonal==1 and nonseasonal == 1):\n",
    "        temp = np.concatenate([np.array(xi),np.array(data)])\n",
    "        temp_ = temp.dropna()\n",
    "        \n",
    "        initial_seasonal_delta = xi.iloc[season]-xi.iloc[0]\n",
    "        s_diffed_cat = np.concatenate([[np.array(initial_seasonal_delta)],np.array(temp_)])\n",
    "\n",
    "        ns_undiffed = diff_inv(s_diffed_cat,1,1)[-len(temp_.dropna()):]\n",
    "\n",
    "        ns_diffed_cat = np.concatenate([np.array(xi[1:(season+1)]),np.array(ns_undiffed)])\n",
    "        s_undiffed = diff_inv(ns_diffed_cat,season,1)[-len(ns_undiffed):]\n",
    "\n",
    "        return(s_undiffed[-len(data):])\n",
    "\n",
    "    #non seasonal\n",
    "    #undiff(temp_test['target'].dropna(), 0, nonseasonal,[raw_int[chosen].loc[temp_train['target'].index[-1]]])\n",
    "    #test\n",
    "    #undiff(temp_test[chosen],seasonal,nonseasonal,raw_int[chosen].loc[temp_train['target'].index[-1:]])\n",
    "    #train\n",
    "    #train_prior_date = raw_int[chosen].index[np.argwhere(data_final.index==temp_train['target'].index[0]).ravel()[0]-nonseasonal]\n",
    "    #train_xi=[raw_int[chosen].loc[train_prior_date]]\n",
    "    #undiff(temp_train['target'], seasonal, nonseasonal,xi)\n",
    "\n",
    "    #seasonal\n",
    "    #undiff(raw_int[chosen].diff(periods=4).dropna(),1,0,raw_int[chosen][0:season])\n",
    "\n",
    "    #non seasonal and seasonal\n",
    "    #undiff(raw_int[chosen].diff(periods=4).diff().dropna(),seasonal,nonseasonal,raw_int[chosen][0:season+nonseasonal])\n",
    "\n",
    "def difference(data, seasonal, nonseasonal):\n",
    "\n",
    "    if seasonal > 0:\n",
    "        return(data.diff(periods=season).diff(nonseasonal))\n",
    "    elif season > 0:\n",
    "        return(data.diff(nonseasonal))\n",
    "    else:\n",
    "        return(data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8ef083-5f57-410b-8c5a-0b476293b4e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f6b876-1953-4937-bc06-3c05123d53b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e958a51-c9e2-4aa4-a56a-fa7b21fef44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "numCV = 5\n",
    "tscv = TimeSeriesSplit(n_splits = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33a75df-bb32-49a6-9fb6-1275c50118bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8522fc-a4da-4a03-a113-ab647a07910d",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pd.read_csv(\"all_data.csv\",index_col=0)\n",
    "raw.index = pd.to_datetime(raw.index)\n",
    "\n",
    "#fillna(method='bfill')\n",
    "raw_int = raw.interpolate(method='time').dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f46ab1-885b-445b-aad6-d4216c46acd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e381b0b6-e08c-4b8e-ab95-f18116a375e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = train_test_split(raw_int.index, test_size=.33, random_state=0, shuffle=False)\n",
    "\n",
    "test_sets = []\n",
    "\n",
    "for i in indexes:\n",
    "    test_sets.append(raw_int.index.difference(i))\n",
    "    \n",
    "training = indexes[0]\n",
    "testing = indexes[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be31cb10-77cb-49e6-95fe-daf9070147a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9539ddc5-f4d2-405a-b6b5-463192afa593",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delta = (raw_int-raw_int.shift()).dropna()\n",
    "#raw_delta = (raw_int - raw_int.apply(lag,0)).dropna()\n",
    "#raw_delta.head()\n",
    "\n",
    "#raw_delta.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cd80bd-da12-4330-bb5d-fa81b6a9eac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215c3655-065a-4b5f-bf22-1f20181c718e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#for i in range(0,len(raw_int.columns)):\n",
    "        \n",
    "#np.max(sndif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb95b5fa-44cc-4ea7-adb1-7f21bd8b1f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_int.diff().dropna().apply(pmdarima.arima.nsdiffs(m=4))\n",
    "\n",
    "sndif = []\n",
    "\n",
    "season = 4\n",
    "maxn = season\n",
    "\n",
    "npa = []\n",
    "\n",
    "for s in range(0,len(raw_int.columns)):\n",
    "    npa.append([raw_int.columns[s],maxn,training])\n",
    "    \n",
    "sndif = clientFunction(sndif_,npa)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2129f972-961c-4202-aae0-d6d9b3445422",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sndif_(npa[0])\n",
    "#sndif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee439e23-992e-4dce-8cad-524c2f95eed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndif = []\n",
    "\n",
    "npa = []\n",
    "\n",
    "for s in range(0,len(raw_int.columns)):\n",
    "    npa.append([raw_int.columns[s],training])\n",
    "    \n",
    "ndif = clientFunction(ndif_,npa)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8556689a-0f4e-4111-a5fa-30da2d1985da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52aaa60d-8804-4476-af02-bc5813637247",
   "metadata": {},
   "outputs": [],
   "source": [
    "#doesn't preserve na's...\n",
    "#len(pmdarima.utils.diff(temp,1,1).ravel())\n",
    "\n",
    "deseasoned = pd.DataFrame()\n",
    "for i in range(0,len(raw_int.columns)):\n",
    "    if(sndif[i]*season == 0):\n",
    "        temp = raw_int.iloc[:,[i]]\n",
    "    else:\n",
    "        temp = raw_int.iloc[:,[i]]\n",
    "        if(sndif[i]>0):\n",
    "            for d in range(0,sndif[i]):\n",
    "                temp = pd.DataFrame(temp.values.ravel()-lagpad(temp.values.ravel(),1*season)).set_index(temp.index)\n",
    "                temp.columns = raw_int.iloc[:,[i]].columns\n",
    "    deseasoned = pd.concat([deseasoned,temp],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f2455f-2517-4844-a92d-f0870c4ebfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for d in deseasoned.columns:\n",
    "    #print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3876d330-9fc9-484b-8be9-562cebe91ef3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211bbabb-94ff-47b2-8ee3-56fe70ff46c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "deseasoned_differenced = pd.DataFrame()\n",
    "\n",
    "for i in range(0,len(raw_int.columns)):\n",
    "    temp_ = deseasoned.iloc[:,[i]]\n",
    "    colnames = temp_.columns\n",
    "    if ndif[i]>0:\n",
    "        #print(ndif[i])\n",
    "        for d in range(0,ndif[i]):\n",
    "            #print(d)\n",
    "            #\n",
    "            #print(temp_.columns)\n",
    "            #temp_ = pd.DataFrame(temp_.values.ravel()-lagpad(temp_.values.ravel(),1)).set_index(temp_.index)\n",
    "            #temp_.columns = colnames\n",
    "            temp = temp_.diff()\n",
    "    temp.columns = temp_.columns\n",
    "    deseasoned_differenced = pd.concat([deseasoned_differenced,temp],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0674d6d-026c-4d0f-bfb5-6d076165a3c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6874c8-c189-422d-8743-6d398590d6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for d in deseasoned_differenced.columns:\n",
    "    #print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f309a8f4-665b-473b-b35f-ceb1cbcb7246",
   "metadata": {},
   "outputs": [],
   "source": [
    "deseasoned_differenced.interpolate(method='time').isna().sum().sum()\n",
    "#.fillna(method='bfill')\n",
    "#raw_int = raw.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a7c307-5b10-4e14-936c-3665c4579e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "deseasoned_differenced.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfebf96-1bd9-41ed-855d-7ada55f8682f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for d in deseasoned_differenced.columns:\n",
    "    #print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445377f0-481e-4147-8396-662f98b29fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://machinelearningmastery.com/time-series-data-stationary-python/\n",
    "cleaned = deseasoned_differenced.interpolate(method='time')#.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bc1b23-ea20-462b-a7eb-187202d5254a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(cleaned.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3c28a1-d38f-4a76-a0e4-6867ccd7fdf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca41a320-0a65-4655-a883-7979a533bba8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fb0918-211c-4e91-96d2-ed65662ba25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many are stationary?\n",
    "%matplotlib inline\n",
    "pd.DataFrame(cleaned.dropna().apply(adfuller).iloc[1,]).iloc[:,0].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44357db8-49cd-4001-b834-7708f0b0a4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for d in cleaned.columns:\n",
    "    #print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a64a2b4-4999-44c7-a16a-022499effcce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e6f0e4-00b9-4d1f-a0d5-c96148ba8280",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.cumsum(x_names=='BOGZ1FL105015105Q')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10040ac7-710f-4822-ac65-dd624ea1cd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(cleaned.iloc[:,raw.columns=='BACDINA066MNFRBNY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170f5bde-8c64-44d0-97c6-955504a29dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for c in cleaned.columns:\n",
    "    #print(y_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fb03f9-0620-4717-aa8b-d42d540bac92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3842ba-3b4a-4abb-ac5d-2a210de4d076",
   "metadata": {},
   "outputs": [],
   "source": [
    "ccf_max_lag = 4\n",
    "\n",
    "ccf_ = []\n",
    "\n",
    "npa = []\n",
    "\n",
    "#chosen = cleaned.columns[random.randint(0,len(cleaned.columns)-1)]\n",
    "chosen = 'MSPUS'\n",
    "chosen = 'LXXRCSA'\n",
    "print(chosen)\n",
    "y_name = cleaned.columns[cleaned.columns==chosen].values[0]\n",
    "#x_names = cleaned.columns[(cleaned.columns!=cleaned.columns[0])]\n",
    "x_names = cleaned.columns\n",
    "\n",
    "for s in range(0,len(cleaned.columns)):\n",
    "    #y_name = y_name_\n",
    "    x_name = x_names[s]\n",
    "    #print(x_name)\n",
    "    npa.append([y_name,x_name,training])\n",
    "    \n",
    "ccf_ = clientFunction(ret_ccf,npa)\n",
    "\n",
    "y = np.array(cleaned.iloc[:,cleaned.columns==y_name]).ravel()\n",
    "x = y\n",
    "#last one is for comparing with itself to ensure 0 lag ccf is 1\n",
    "ccf_.append([y_name,y_name,crosscorrelation(x,y, ccf_max_lag, mode='corr')])\n",
    "#ccf_.append([np.array([y_name,y_name]).reshape(2,1),crosscorrelation(x,y, 4, mode='corr')])#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4d08d6-1f0c-4447-be2d-a3a6b91a1fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(cleaned[chosen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9e6ab6-e639-4dce-8db4-e29a2d729119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4440f99-f158-43d1-9108-6967fc0122aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "range_ = [*range(-ccf_max_lag,ccf_max_lag+1)].copy()\n",
    "\n",
    "ccf_scores = pd.DataFrame()\n",
    "\n",
    "for c in ccf_:\n",
    "    #print(c)\n",
    "    y = c[0]\n",
    "    x = c[1]\n",
    "    #print(x)\n",
    "    ar_ = pd.DataFrame(c[2])\n",
    "    #print(ar_)\n",
    "    ar_.index = range_\n",
    "    ar_.columns = [x]\n",
    "    #abs(ar_)\n",
    "    ccf_scores = pd.concat([ccf_scores,ar_],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df77cb1-9235-4db8-ab3e-826f65c44330",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0058905-36bc-476c-b52a-b8fde036eb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(ccf_scores.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96543fec-bd07-4bbf-b3d0-b1431bcba01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#derive optimally lagged dataset\n",
    "\n",
    "data_final = pd.DataFrame()\n",
    "\n",
    "best_lags = []\n",
    "#don't want the last one\n",
    "for c in ccf_scores.columns[:-1]:\n",
    "    #print(c)\n",
    "    temp = ccf_scores[ccf_scores.index>0][c]\n",
    "    bl = ccf_scores.index[ccf_scores.index>0][np.argmax(abs(temp))]\n",
    "    best_lags.append(bl)\n",
    "    data = pd.DataFrame(lagpad(cleaned[c],bl))\n",
    "    data.index = cleaned[c].index\n",
    "    data.columns = [c]\n",
    "    data_final = pd.concat([data_final,data],axis=1)\n",
    "    \n",
    "#doesn't need to be shift, because all other values are offset by at least 1\n",
    "temp = pd.DataFrame(cleaned[chosen])\n",
    "temp.columns = ['target']\n",
    "data_final = pd.concat([temp,data_final],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b577961c-4929-4071-8f22-97381fead1e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224de44f-3fc4-4630-85ac-622a7d8d3c2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a35a30-937e-4751-836a-8cf447037e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_final_dask = dd.from_pandas(data_final,npartitions=128)\n",
    "#data_final_dask_w_y = dd.concat([cleaned[[y_name]],data_final_dask.compute()],axis=1)\n",
    "#names_ = ['target']\n",
    "#names_.extend(cleaned.columns)\n",
    "#data_final_dask_w_y.columns = names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cc2b44-72f9-46b4-b3fc-a10c46fd0c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_final_dask.apply(np.cumsum,axis=1).compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce954cbf-4eca-4d46-a2af-afa25e3e88f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac13186e-5615-44b8-8593-067501c7fcb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea2c216-e4cc-4ce5-aead-bcb283cd801c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(data_final.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d33c54e-9bd1-4012-b11a-6a28f1cc8198",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(raw_int[chosen])\n",
    "#plt.plot(cleaned[chosen])\n",
    "#plt.plot(data_final_dask_w_y[chosen].compute())\n",
    "#plt.plot(data_final_dask_w_y[['target']].compute().loc[training])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2259a1-3d45-43b5-9f1a-4715ed7adf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.sort(data_final_dask_w_y.compute().isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a92423-63d4-4d38-85ed-dc9e910eef5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17484b9-be53-4ab5-ae8c-1b43b45e4ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "zca1_subset = working_set[subset.columns.difference(['target',max_pname])].dropna()\n",
    "zca1 = zca.fit(zca1_subset)\n",
    "zca1_df = pd.DataFrame(zca1.transform(zca1_subset))\n",
    "zca1_df.columns = zca1_subset.columns\n",
    "zca1_df.index = zca1_subset.index\n",
    "\n",
    "zca1_subset = working_set[subset.columns.difference(['target',max_pname])].dropna()\n",
    "zca1 = zca.fit(zca1_subset)\n",
    "zca1_df = pd.DataFrame(zca1.transform(zca1_subset))\n",
    "zca1_df.columns = zca1_subset.columns\n",
    "zca1_df.index = zca1_subset.index\n",
    "#pd.concat([target,zca1],axis=1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cd5876-0407-4d06-adc3-bb6e5d3ad84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620eabbd-bca0-4db4-a177-911270eeb318",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = y_subset(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693debdc-01f1-4e2e-9a79-8e80f1b885eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b784a9c4-e35a-45f8-9cc6-779a645bd20c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d705c499-58c2-4e07-a2b5-9a32d69e67ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#.compute().loc[training],training,testing\n",
    "temp_train = data_final.loc[training].dropna()\n",
    "temp_test = data_final.loc[testing].dropna()\n",
    "\n",
    "drop = temp_train.columns[temp_train.apply(np.std)==0]\n",
    "\n",
    "temp_train.drop(drop,axis=1,inplace=True)\n",
    "\n",
    "s_f_s = regress(temp_train)\n",
    "\n",
    "target = temp_train.columns[0]\n",
    "#.fit is X, y format\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    fitted = s_f_s.fit(temp_train.loc[:, ~temp_train.columns.isin(['target'])], pd.DataFrame(temp_train['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a6ddca-6ce8-43af-a504-6b89ab6a479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_train['target']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca32a436-fbcb-4c94-8cf9-fdbce9501c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_test['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5932a78-b263-4908-a31a-c71cb98b8041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#define F-test function\n",
    "def f_test(x, y):\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    f = np.var(x, ddof=1)/np.var(y, ddof=1) #calculate F test statistic \n",
    "    dfn = x.size-1 #define degrees of freedom numerator \n",
    "    dfd = y.size-1 #define degrees of freedom denominator \n",
    "    p = 1-scipy.stats.f.cdf(f, dfn, dfd) #find p-value of F test statistic \n",
    "    return f, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b891ac7d-0c99-4475-b6bf-ae6a793a5676",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_test(temp_train['target'],temp_test['target'])\n",
    "import scipy.stats as stats\n",
    "stats.f_oneway(temp_train['target'],temp_test['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326d49e3-fa78-4df3-a217-037fb207e2d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19ed9f7-ee6e-407c-9206-8e3a04605112",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plot_sfs(fitted.get_metric_dict(), kind='std_err')\n",
    "plt.title('Sequential Forward Selection (w. StdErr)')\n",
    "plt.savefig(str(target)+'.png', dpi=300, format='png', bbox_inches='tight')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96987af-87b6-4dc1-9764-bdf1c85d6e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7db84a3-b092-4b34-a833-dc4061cd68b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_table = pd.DataFrame(fitted.get_metric_dict()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6034af74-a848-424b-8143-4a31abe3cf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413f0cea-63fd-430f-b4e7-b1be3dfff54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "winners = metric_table[metric_table['avg_score']<(np.std(metric_table['avg_score'])+np.min(metric_table['avg_score']))].tail(1)['feature_names'].index\n",
    "winners_ = np.asarray(fitted.get_metric_dict()[winners[0]]['feature_names'])\n",
    "\n",
    "knee_last = np.min(np.where(np.round([*metric_table['avg_score']],6)==np.round(np.max([*metric_table['avg_score']]),6)))\n",
    "\n",
    "#elbow method beats the other method\n",
    "temp_df = findknee(np.array(metric_table['avg_score'][0:(knee_last+1)]))\n",
    "winners_ = np.asarray(metric_table.iloc[np.min(np.where([temp_df==np.max(temp_df)])[1])]['feature_names'])\n",
    "#winners_ = np.asarray(metric_table.iloc[10]['feature_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d72e16-2442-4265-af9d-9eb7010d47c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce01763-dd51-436d-9966-a7d84d580005",
   "metadata": {},
   "outputs": [],
   "source": [
    "winners_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ef0162-f7ce-44a1-996c-8d1bc64690d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c8050c-e9cf-4c1b-ac6b-da5a86ea71b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(metric_table['avg_score'][0:(knee_last+1)]))\n",
    "\n",
    "metric_table.loc[metric_table['avg_score']<=np.std(abs(metric_table['avg_score'][0:(knee_last+1)]))+np.min(abs(metric_table['avg_score'][0:(knee_last+1)]))+np.min((metric_table['avg_score'][0:(knee_last+1)]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a63fce-3966-42d9-b9d2-fdfebafabfee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9495fe14-a982-4b2c-abd7-c2bbf3118c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = temp_train[winners_]\n",
    "\n",
    "y_train = temp_train['target']\n",
    "\n",
    "sig_table = np.zeros(shape=(len(X_train.columns)))\n",
    "signs_table = np.zeros(shape=(len(X_train.columns)))\n",
    "kfold = KFold(n_splits=numCV, shuffle=True)\n",
    "\n",
    "#used for pcorr kfolds as well as best subsets\n",
    "train_ = []\n",
    "test_ = []\n",
    "\n",
    "kfold.get_n_splits(X_train.index)\n",
    "\n",
    "for train_indices, test_indices in kfold.split(X_train.index):\n",
    "    train_.append(train_indices)\n",
    "    test_.append(test_indices)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae0b22a-472d-44b2-ac33-89925cfd350a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816b4d23-3ada-45e5-a822-3a97547fd548",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = .05\n",
    "\n",
    "set_ = list(winners_)\n",
    "\n",
    "max_pvalue = 1\n",
    "\n",
    "subset = temp_train[np.concatenate([['target'],winners_])]\n",
    "\n",
    "n=len(subset)\n",
    "\n",
    "while(max_pvalue>=threshold):\n",
    "\n",
    "    dist = scipy.stats.beta(n/2 - 1, n/2 - 1, loc=-1, scale=2)\n",
    "    p_values = pd.DataFrame(2*dist.cdf(-abs(subset.pcorr()['target']))).T\n",
    "    p_values.columns = list(subset.columns)\n",
    "\n",
    "    max_pname = p_values.idxmax(axis=1)[0]\n",
    "    max_pvalue = p_values[max_pname].values[0]\n",
    "\n",
    "    print(max_pvalue, max_pname)\n",
    "\n",
    "    #to prevent errors, always return 1\n",
    "    if len(set_)==1:\n",
    "        break\n",
    "\n",
    "    if (max_pvalue > threshold):\n",
    "\n",
    "        set_.remove(max_pname)\n",
    "        temp = [target]\n",
    "        temp.extend(set_)\n",
    "        subset = subset[temp]\n",
    "\n",
    "        max_pname=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b3c525-303a-4ba7-8ed9-286852a10e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75972a79-5470-4ca4-8d06-386cd811b53d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd757a6a-8c83-473d-9f63-f3d3f50fb6b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fd52b1-00ad-4862-81f4-79bf6dbe11c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(metric_table['avg_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6b1251-fdbf-46a9-811e-0f1c68556e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_train[np.concatenate([['target'],set_])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b962e69-fda2-487a-aeba-ccbd1f4d5dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in set_:\n",
    "    plt.scatter(temp_train[['target']], temp_train[[w]])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf87fc6c-2463-43b6-8417-3a1e6d1b9628",
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp_train[list('target').extend(winners_)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f69fbff-f686-440c-b2e9-1571f869bf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(best_lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99aed22-b692-4bb1-b222-9468ed5458cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952dd413-18a6-469a-ad1f-3fce72f90b94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c44de9c-f18b-408a-a57a-a1a8e708089c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2354f090-86ea-49ea-affe-b681d4afe8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lagposition = []\n",
    "lagatposition = []\n",
    "for s in range(0,len(winners_)):\n",
    "    lagposition.append(np.where(ccf_scores.columns==winners_[s])[0][0])\n",
    "    lagatposition.append(best_lags[np.where(ccf_scores.columns==winners_[s])[0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75430906-e4a9-4c16-9d68-568c55e91465",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6d7c95-affb-4eb8-9c8b-dbea701ec24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lagatposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2430ee8f-f974-47da-9f6e-d9193f067d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sm.add_constant(temp_train[winners_])\n",
    "results = sm.OLS(temp_train['target'],X).fit()\n",
    "print(results.summary())\n",
    "\n",
    "inverses = []\n",
    "\n",
    "print(chosen)\n",
    "\n",
    "value = np.where(raw_int.columns==chosen)[0][0]\n",
    "\n",
    "#print(\n",
    "print('sndif:', sndif[value])\n",
    "print('ndif:', ndif[value])\n",
    "inverses.append([chosen,sndif[value],ndif[value]])\n",
    "\n",
    "train_forecast = results.predict(temp_train[np.concatenate([['target'],winners_])])\n",
    "test_forecast = results.predict(temp_test[np.concatenate([['target'],winners_])])\n",
    "\n",
    "print(MAPE(temp_test['target'],test_forecast))\n",
    "\n",
    "index_ = []\n",
    "\n",
    "for w in winners_:\n",
    "    print(w)\n",
    "    value = np.where(raw_int.columns==w)[0][0]\n",
    "    index_.append(value)\n",
    "    inverses.append([w,sndif[value],ndif[value]])\n",
    "    print('lags:', best_lags[np.where(ccf_scores.columns==winners_[s])[0][0]])\n",
    "    print('sndif:', sndif[value])\n",
    "    print('ndif:', ndif[value])\n",
    "\n",
    "%matplotlib inline\n",
    "corrMatrix = pd.concat([temp_train['target'],temp_train[winners_]],axis=1).corr().sort_values(kind=\"quicksort\", by='target', ascending=False,key=abs)\n",
    "sn.heatmap(corrMatrix, annot=True)\n",
    "plt.show()\n",
    "\n",
    "pcorrMatrix = pd.concat([temp_train['target'],temp_train[winners_]],axis=1).pcorr().sort_values(kind=\"quicksort\", by='target', ascending=False,key=abs)\n",
    "sn.heatmap(pcorrMatrix, annot=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e60828-f399-431a-9be8-b42b1340c578",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in winners_:\n",
    "    plt.scatter(temp_train[['target']], temp_train[[w]])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edab7542-543b-4518-bcb4-8aa2de26c70f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffebf39-6fb8-458e-8ea0-20f8424c6456",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(temp_train))\n",
    "print(len(temp_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9a72a8-c280-4e6d-8b3d-c8d98ac73219",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for w in winners_:\n",
    "sn.set_theme(style=\"ticks\")\n",
    "\n",
    "t = temp_train[np.concatenate([['target'],winners_])]\n",
    "t_ = t[['target']]\n",
    "t_.columns = ['target2']\n",
    "#t_.rename('target2')\n",
    "t = pd.concat([t,t_],axis=1)\n",
    "sn.pairplot(t, hue=\"target\")\n",
    "    #plt.scatter(temp_train[['target']], temp_train[[w]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f623afd-39d9-44bf-b185-f0e0071fe26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonal = inverses[0][1]\n",
    "print(seasonal)\n",
    "nonseasonal = inverses[0][2]\n",
    "print(nonseasonal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9545641-8a05-4190-920f-f8eb509bd30f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900eac71-84c7-4e54-989b-2f78c99aace5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71755a17-dfa2-479d-a3f9-d91134239c45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508fe348-072f-4814-a7c3-af7f24cf7f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "newindex = []\n",
    "\n",
    "for i in range(0,len(temp_train.index)):\n",
    "    newindex.append(temp_train.index[i])\n",
    "\n",
    "for i in range(0,nonseasonal):\n",
    "    newindex.append(last_day_of_month(temp_train.index[-1] + pd.DateOffset(90*i)))\n",
    "    \n",
    "len(newindex)\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fa1c1d-2570-4358-9aae-d501ba283957",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c92d06-1f79-434f-a853-feee79505a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def undiff(data, seasonal, nonseasonal, xi):\n",
    "    \n",
    "    print(\"you have to know what xi for which use case you are going to use\")\n",
    "    \n",
    "    #nonseasonal\n",
    "    if(nonseasonal!=0 and seasonal==0):\n",
    "        temp = np.concatenate([np.array(xi),np.array(data)])\n",
    "        temp_ = diff_inv(temp,1,nonseasonal)\n",
    "        return(temp_[-len(data):])\n",
    "        \n",
    "    #seasonal\n",
    "    if(seasonal!=0 and nonseasonal == 0):\n",
    "        temp = np.concatenate([np.array(xi),np.array(data)])\n",
    "        temp_ = diff_inv(temp,season,1)\n",
    "        return(temp_[-len(data):])\n",
    "    \n",
    "    #both\n",
    "    if(seasonal==1 and nonseasonal == 1):\n",
    "        \n",
    "        '''\n",
    "        temp = np.concatenate([np.array(xi),np.array(data)])\n",
    "        #print(temp)\n",
    "        temp_ = temp\n",
    "        \n",
    "        print(type(xi))\n",
    "        initial_non_seasonal_delta = xi.iloc[season+nonseasonal]-xi.iloc[season]\n",
    "        print(initial_non_seasonal_delta)\n",
    "        \n",
    "        initial_seasonal_delta = xi.iloc[season]-xi.iloc[0]\n",
    "        #print(initial_seasonal_delta)\n",
    "        s_diffed_cat = np.concatenate([[np.array(initial_seasonal_delta)],np.array(temp_)])\n",
    "\n",
    "        ns_undiffed = diff_inv(s_diffed_cat,1,1)[-len(temp_):]\n",
    "\n",
    "        ns_diffed_cat = np.concatenate([np.array(xi[1:(season+1)]),np.array(ns_undiffed)])\n",
    "        s_undiffed = diff_inv(ns_diffed_cat,season,1)[-len(ns_undiffed):]\n",
    "\n",
    "        return(s_undiffed[-len(data):])\n",
    "        '''\n",
    "                #temp = data\n",
    "\n",
    "        temp_ = np.concatenate([np.array(xi),np.array(data)])\n",
    "        \n",
    "        initial_seasonal_delta = xi.iloc[season]-xi.iloc[0]\n",
    "        s_diffed_cat = np.concatenate([[np.array(initial_seasonal_delta)],np.array(data.dropna())])\n",
    "\n",
    "        ns_undiffed = diff_inv(s_diffed_cat,1,1)[-len(temp_):]\n",
    "\n",
    "        ns_diffed_cat = np.concatenate([np.array(xi[1:(season+1)]),np.array(ns_undiffed)])\n",
    "        s_undiffed = diff_inv(ns_diffed_cat,season,1)[-len(ns_undiffed):]\n",
    "\n",
    "        return(s_undiffed[-len(data):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e01950-56b2-47a2-912f-d14205be653b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d7b73e-53f9-40eb-b071-b80cb8361155",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9f6b90-3aa9-4ae8-afc2-e3c386d8ecc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prior_date = raw_int[chosen].index[np.argwhere(data_final.index==temp_train['target'].index[0]).ravel()[0]-nonseasonal-(seasonal*season)]\n",
    "train_prior_date_1 = raw_int[chosen].index[np.argwhere(data_final.index==temp_train['target'].index[0]).ravel()[0]-1]\n",
    "\n",
    "train_xi=raw_int[chosen].loc[train_prior_date:train_prior_date_1]\n",
    "\n",
    "undiffed_train = pd.DataFrame(undiff(temp_train['target'], seasonal, nonseasonal, train_xi),columns=['target']).set_index(temp_train.index)\n",
    "undiffed_train_forecast = pd.DataFrame(undiff(train_forecast, seasonal, nonseasonal, train_xi),columns=['target']).set_index(temp_train.index)\n",
    "\n",
    "test_prior_date = raw_int[chosen].index[np.argwhere(data_final.index==temp_test['target'].index[0]).ravel()[0]-nonseasonal-(seasonal*season)]\n",
    "test_prior_date_1 = raw_int[chosen].index[np.argwhere(data_final.index==temp_test['target'].index[0]).ravel()[0]-1]\n",
    "\n",
    "test_xi=raw_int[chosen].loc[test_prior_date:test_prior_date_1]\n",
    "\n",
    "#[raw_int[chosen].loc[temp_train['target'].index[-1]]]\n",
    "undiffed_test_forecast = pd.DataFrame(undiff(test_forecast, seasonal, nonseasonal,test_xi),columns=['target']).set_index(temp_test.index)\n",
    "undiffed_test = pd.DataFrame(undiff(temp_test['target'].dropna(), 0, nonseasonal,test_xi),columns=['target']).set_index(temp_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daeeb70c-1575-4af9-9612-8071be089962",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d9ad5d-8276-42d5-80e2-936b96329886",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(pd.concat([raw_int.loc[temp_train.index][[chosen]],pd.DataFrame(undiff(temp_train['target'], seasonal, nonseasonal, train_xi),columns=['target']).set_index(temp_train.index)],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb18675-ac53-4a62-8401-580f8f96f484",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(raw_int[chosen])\n",
    "plt.show()\n",
    "plt.plot(pd.concat([undiffed_train,undiffed_test],axis=0))\n",
    "plt.show()\n",
    "plt.plot(pd.concat([undiffed_train_forecast,undiffed_test_forecast],axis=0))\n",
    "plt.show()\n",
    "plt.plot(pd.concat([undiffed_train,undiffed_train_forecast],axis=1))\n",
    "plt.show()\n",
    "plt.plot(pd.concat([undiffed_test,undiffed_test_forecast],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12af90d-66c6-4441-9279-604725aaca0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "#stats.f_oneway(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e144ae44-bcc2-4218-a240-5529ddf451a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(temp_train['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de7394d-a5df-4129-918f-dfcda4681d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(temp_test['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b31f01-c16b-4eab-86c1-3ff8a3292ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ccf_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc38d410-94ca-4354-986a-ddd53920b7af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fd4ded-28aa-450d-8da8-cd9ef56757b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(ccf_scores.columns==set_[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fe0c34-945c-4d88-8b69-872ef9a7bf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(best_lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea06832-da34-424d-9019-875c7040cde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sm.add_constant(data_final.dropna()[set_])\n",
    "results = sm.OLS(data_final.dropna()[['target']],X).fit()\n",
    "print(results.summary())\n",
    "\n",
    "for s in range(0,len(set_)):\n",
    "    print(np.where(ccf_scores.columns==set_[s])[0][0])\n",
    "    \n",
    "inverses = []\n",
    "\n",
    "print(chosen)\n",
    "\n",
    "value = np.where(raw_int.columns==chosen)[0][0]\n",
    "\n",
    "#print(\n",
    "print('sndif:', sndif[value])\n",
    "print('ndif:', ndif[value])\n",
    "inverses.append([chosen,sndif[value],ndif[value]])\n",
    "\n",
    "train_forecast = results.predict(temp_train[np.concatenate([['target'],set_])])\n",
    "test_forecast = results.predict(temp_test[np.concatenate([['target'],set_])])\n",
    "\n",
    "print(MAPE(temp_test['target'],test_forecast))\n",
    "\n",
    "index_ = []\n",
    "\n",
    "for w in set_:\n",
    "    print(w)\n",
    "    value = np.where(raw_int.columns==w)[0][0]\n",
    "    index_.append(value)\n",
    "    inverses.append([w,sndif[value],ndif[value]])\n",
    "    print('lags:', best_lags[np.where(ccf_scores.columns==set_[s])[0][0]])\n",
    "    print('sndif:', sndif[value])\n",
    "    print('ndif:', ndif[value])\n",
    "\n",
    "%matplotlib inline\n",
    "corrMatrix = pd.concat([temp_train['target'],temp_train[set_]],axis=1).corr().sort_values(kind=\"quicksort\", by='target', ascending=False,key=abs)\n",
    "sn.heatmap(corrMatrix, annot=True)\n",
    "plt.show()\n",
    "\n",
    "pcorrMatrix = pd.concat([temp_train['target'],temp_train[set_]],axis=1).pcorr().sort_values(kind=\"quicksort\", by='target', ascending=False,key=abs)\n",
    "sn.heatmap(pcorrMatrix, annot=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51702ec-ba0b-48f1-bdc5-6f22cae90e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "\n",
    "future = client.map(regress, X)\n",
    "\n",
    "results = []\n",
    "best = -1\n",
    "for f in as_completed(future):\n",
    "    results.append(f.result())\n",
    "    \n",
    "def y_subset(df):\n",
    "    \n",
    "    X = list ()\n",
    "    \n",
    "    for var_pos in range(0,len(df.columns)):\n",
    "        variables=df.columns\n",
    "        target=variables[var_pos]\n",
    "        #print(target)\n",
    "        #print(variables.isin([target]))\n",
    "        temp = pd.concat([pd.DataFrame(df[target]),df_.loc[:, ~df.columns.isin([target])]],axis=1)\n",
    "        #print(temp)\n",
    "        X.append(temp)\n",
    "    return(X)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f0d63e-85ca-449b-8390-aeeecb71313f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "scaler = StandardScaler()\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits = 5)\n",
    "\n",
    "#scaler.fit(np.array(data_final_dask_w_y[['target']].compute().loc[training]).reshape(-1, 1))\n",
    "\n",
    "New_Names = list(data_final_dask_w_y.columns.difference(['target']))\n",
    "\n",
    "outer_dataset = data_final_dask_w_y.compute().loc[training].dropna()\n",
    "target = outer_dataset[['target']]\n",
    "\n",
    "subset = pd.concat([target,outer_dataset[New_Names]],axis=1)\n",
    "\n",
    "num_folds = 2\n",
    "#kfold = KFold(n_splits=num_folds, shuffle=False)\n",
    "#train, test = kfold.get_n_splits(outer_dataset.index)\n",
    "\n",
    "p_threshold = .05\n",
    "\n",
    "iteration = 0\n",
    "max_pvalue = 1\n",
    "\n",
    "while(max_pvalue>=.05):\n",
    "#\n",
    "    print(chosen)\n",
    "    \n",
    "    print(New_Names)\n",
    "    \n",
    "    n_p_values = pd.DataFrame()\n",
    "\n",
    "    p_values = []\n",
    "    \n",
    "    #parallelize here (x16)\n",
    "    for n in New_Names:\n",
    "        #print(n)\n",
    "        New_Names_testing = list(np.array(New_Names)[(np.array(New_Names)!=n)])\n",
    "        p_values.append(pvalues(n))\n",
    "        \n",
    "    p_values_df = pd.DataFrame(p_values,index=New_Names)\n",
    "    print(p_values_df)\n",
    "\n",
    "    max_pname = New_Names[np.argmax(p_values_df)]\n",
    "    max_pvalue = p_values[np.argmax(p_values_df)]\n",
    "\n",
    "    #n_p_values = pd.concat([n_p_values,p_values],axis=0)\n",
    "    #print(n_p_values)\n",
    "\n",
    "    if (max_pvalue > .05):\n",
    "        print([max_pname, max_pvalue])\n",
    "        #New_Names.remove(max_pname)\n",
    "        #New_Names_testing = list(np.array(New_Names_testing)[(np.array(New_Names_testing)!=max_pname)])\n",
    "        New_Names = list(np.array(New_Names)[(np.array(New_Names)!=max_pname)])\n",
    "        temp = ['target']\n",
    "        temp.extend(New_Names)\n",
    "        subset = subset[temp]\n",
    "    print()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40460a2a-3b85-4e98-a95e-792e089b57ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "restartClientFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c159e1b-117f-48fc-87f8-b402c6310ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_temp = data_final_dask_w_y.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f421c1-f54c-4de0-a658-f83efe327bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset[New_Names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340c1eec-b1c3-4ec2-9c10-4cdd03192cf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7523b67a-6bd8-46a1-91e7-345d9fc2f12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "includes = []\n",
    "for c in subset.columns:\n",
    "    index = np.argwhere(data_final_dask_w_y.columns==c)[0][0]\n",
    "    includes.append(index)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db12096a-43ea-4eff-aee9-1c47bb6230cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b53efc9-e332-4828-b7b3-a8f3cf70118b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reg = train(data_final_dask_w_y[subset.columns].compute().loc[training].dropna())\n",
    "#subset.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572f337a-3486-4c7e-b469-74ce5d877360",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reg = regress(data_final_dask_w_y[subset.columns].compute().dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22abd359-b2e1-4af7-9a7d-8a3d17434be0",
   "metadata": {},
   "source": [
    "#### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
